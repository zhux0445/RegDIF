---
title: "DIF Detection via Regularization"
author: "Ruoyi Zhu"
date: "9/9/2020"
output:  bookdown::pdf_document2
header-includes:
  - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
---

# Conventional DIF Detection Methods

## Likelihood Ratio Test

In likelihood ratio test, DIF will be tested one item at a time. The null hypothesis is that all but the anchor items are DIF. To test whether the *j*th item is DIF, the alternative hypothesis is that the anchor items and the *j*th item is DIF free, and all other items are DIF. If the null hypothesis is rejected (adjust p-value <0.05), the tested item will be flagged as DIF. Since we are testing several hypotheses, multiple comparisons need to be performed. The adjust p-values are the family-wise error rate. We assume the latent traits in reference and focal groups follow normal distributions. The mean and variance of reference group are fixed to be 0 and 1, respectively. The means and variances of focal groups can be freely estimated.

# Multidimensional 2PL Model with DIF

Assume a total sample size $N=N_1+N_2+N_3$, where $N_1$, $N_2$ and $N_3$ are sample sizes of the reference group and two focal groups, respectively. Also assume a test length *m* and trait dimension *q*. For a dichotomously scored item *j*, the probability that examinee *i* with ability vector $\boldsymbol\theta_i$ giving a correct response to item *j* is

\begin{equation}
\begin{aligned}
P_{j}(\boldsymbol\theta_i)  = \frac{1}{1+e^{-({\textbf a_j^T}{\boldsymbol\theta_i}+d_{j}+(\boldsymbol y_i\boldsymbol\gamma_j)\boldsymbol \theta_i+\boldsymbol y_i\boldsymbol\beta_{j})}}  (i=1,...,N; j=1,2,...,m).
\end{aligned}
\end{equation}

$\boldsymbol y_i$ is a group indicator including all the grouping information related to DIF. $\boldsymbol y_i=(0,0)$ if examinee *i* is in the reference group, $\boldsymbol y_i=(1,0)$ if examinee *i* is in the first focal group and $\boldsymbol y_i=(0,1)$ if examinee *i* is in the second focal group.

The ability vector of the $i$th examinee is $\boldsymbol\theta_{i}$ = ($\theta_{i1}$,$\theta_{i2}$,...,$\theta_{ir}$,...,$\theta_{iq}$$)^T$ (i=1,...,N; r=1,2,...,q). $\textbf a_j=(a_{j1},a_{j2})^T$ is the discrimination parameter and $d_j$ is the boundary parameter. $$\boldsymbol \gamma_j =(\boldsymbol \gamma_{j1\cdot},\boldsymbol \gamma_{j2\cdot})^T=\begin{pmatrix}
\gamma_{j11} & \gamma_{j12} & ... &\gamma_{j1r}&...& \gamma_{j1q}\\
\gamma_{j21} & \gamma_{j22} & ... &\gamma_{j2r}&...& \gamma_{j2q}\\
\end{pmatrix} (r=1,...,q)$$ is the non-uniform DIF parameter, where $\boldsymbol \gamma_{j1\cdot}$ is the non-uniform DIF parameter for the first focal group and $\boldsymbol \gamma_{j2\cdot}$ is the non-uniform DIF parameter for the second focal group. $\boldsymbol \beta_j =(\beta_{j1},\beta_{j2})^T$ is the uniform DIF parameter, where $\beta_{j1}$ is the uniform DIF parameter for the first focal group and $\beta_{j2}$ is the uniform DIF parameter for the second focal group. If item *j* does not have DIF, then $\boldsymbol \gamma_j=\boldsymbol 0$ and $\boldsymbol \beta=\boldsymbol 0$. If item *j* has uniform DIF, then $\boldsymbol \gamma_j=\boldsymbol 0$.

Suppose the prior distribution of $\boldsymbol \theta_i$ in group *y* is multivariate normal distribution with mean vector of $\boldsymbol\mu_y$ and covariance matrix of $\boldsymbol\Sigma_y$.
The prior density of $\boldsymbol \theta_i$ is
$$f(\boldsymbol \theta_{i} \mid \boldsymbol\mu_y,\boldsymbol\Sigma_y)=(2\pi)^{-\frac{p}{2}}|\boldsymbol\Sigma_y|^{-\frac{1}{2}}e^{-0.5(\boldsymbol\theta_i-\boldsymbol\mu_y)^T|\boldsymbol\Sigma_y|^{-1}(\boldsymbol\theta_i-\boldsymbol\mu_y)}.$$


If *i* is in 1,...,$N_1$, then $y_i=(0,0)$. $\boldsymbol\mu_{y}=\boldsymbol\mu_1$ and $\boldsymbol \Sigma_y=\boldsymbol \Sigma_1$, $\boldsymbol \theta_i\sim N(\boldsymbol\mu_1,\boldsymbol \Sigma_1)$.

If *i* is in $N_1+1$,...,$N_1+N_2$, then $y_i=(1,0)$. $\boldsymbol\mu_{y}=\boldsymbol\mu_2$ and $\boldsymbol \Sigma_y=\boldsymbol \Sigma_2$, $\boldsymbol \theta_i\sim N(\boldsymbol\mu_2,\boldsymbol \Sigma_2)$.

If *i* is in $N_1+N_2+1$,...,$N_1+N_2+N_3$, then $y_i=(0,1)$. $\boldsymbol\mu_{y}=\boldsymbol\mu_3$ and $\boldsymbol \Sigma_y=\boldsymbol \Sigma_3$, $\boldsymbol \theta_i\sim N(\boldsymbol\mu_3,\boldsymbol \Sigma_3)$.

We set the reference group to have zero means and unit variances, that is, $\boldsymbol\mu_1=\boldsymbol 0$ and diag$(\boldsymbol \Sigma_1)=\boldsymbol 1$. Then with some anchor items, the trait parameters for focal groups, i.e. $\boldsymbol\mu_2$, $\boldsymbol\mu_3$, $\boldsymbol \Sigma_2$ and $\boldsymbol \Sigma_3$, can be freely estimated.

# Graded Response Model with DIF

Assume a total sample size *N*, test length *m*, number of response categories *p* and trait dimension *q*. For a polytomously scored item *j*, the probability that examinee *i* with ability vector $\boldsymbol\theta_i$ reaching level *k* or higher on item *j* is
\begin{equation}
\begin{aligned}
P_{ijk}^*  = \frac{1}{1+e^{-({\textbf a_j}{\boldsymbol\theta_i}+d_{jk}+(\boldsymbol y_i\boldsymbol\gamma_j)\boldsymbol \theta_i+\boldsymbol y_i\boldsymbol\beta_{jk})}}  (i=1,...,N; j=1,2,...,m; k=1,2,...,p-1).
\end{aligned}
\end{equation}

$\boldsymbol y_i$ is a group indicator including all the grouping information related to DIF. $\boldsymbol y_i=(0,0)$ if examinee *i* is in the reference group, $\boldsymbol y_i=(1,0)$ if examinee *i* is in the first focal group and $\boldsymbol y_i=(0,1)$ if examinee *i* is in the second focal group.

\begin{equation}
\begin{aligned}
P_{ijk} = P_{ij,k-1}^* - P_{ijk}^*
\end{aligned}
\end{equation}

is the probability of an examinee *i* with ability vector $\boldsymbol\theta_i$ reaching response level *k* on item *j*.

The trait variable has *q* dimensions. The ability vector of the $i$th examinee is $\boldsymbol\theta_{i}$ = ($\theta_{i1}$,$\theta_{i2}$,...,$\theta_{ir}$,...,$\theta_{iq}$$)^T$ (i=1,...,N; r=1,2,...,q), and the item parameter matrices are

discrimination parameter
$$\textbf A = (\textbf a_{1},\textbf a_{2},...,\textbf a_{j},...,\textbf a_{m})^T=\begin{pmatrix}
a_{11} & a_{12} & ... &a_{1r}&...& a_{1q}\\
a_{21} & a_{22} & ... &a_{2r}&...& a_{2q}\\
\vdots &\vdots & \ddots &\vdots&\ddots& \vdots\\
a_{j1} & a_{j2} & ... &a_{jr}&...& a_{jq}\\
\vdots & \vdots & \ddots & \vdots&\ddots& \vdots\\
a_{m1} & a_{m2} & ... &a_{mr}&... & a_{mq}\\
\end{pmatrix}(j=1,2,...,m;r=1,...,q),$$

boundary parameter
$$\textbf D =  (\textbf d_{1},\textbf d_{2},...,\textbf d_{j},...,\textbf d_{m})^T=\begin{pmatrix}
d_{11} & d_{12} & ... &d_{1k}&...& d_{1,p-1}\\
d_{21} & d_{22} & ... &d_{2k}&...& d_{2,p-1}\\
\vdots &\vdots & \ddots &\vdots&\ddots& \vdots\\
d_{j1} & d_{j2} & ... &d_{jk}&...& d_{j,p-1}\\
\vdots & \vdots & \ddots & \vdots&\ddots& \vdots\\
d_{m1} & d_{m2} & ... &d_{mk}&... & d_{m,p-1}\\
\end{pmatrix}(j=1,2,...,m;k=1,...,p-1),$$

non-uniform DIF parameter
$$\boldsymbol \Gamma = (\boldsymbol \gamma_{1},\boldsymbol \gamma_{2},...,\boldsymbol \gamma_{j},...,\boldsymbol \gamma_{m}) (j=1,...,m)$$

$$\boldsymbol \gamma_j =(\boldsymbol \gamma_{j1\cdot},\boldsymbol \gamma_{j2\cdot})^T=\begin{pmatrix}
\gamma_{j11} & \gamma_{j12} & ... &\gamma_{j1r}&...& \gamma_{j1q}\\
\gamma_{j21} & \gamma_{j22} & ... &\gamma_{j2r}&...& \gamma_{j2q}\\
\end{pmatrix} (r=1,...,q),$$


where *q* is the dimension of $\boldsymbol\theta$, and each row of $\boldsymbol \gamma_{j}$ is (non-uniform) DIF parameter for a focal group, i.e. $\boldsymbol \gamma_{j1\cdot}$ is (non-uniform) DIF parameter for the first focal group, and $\boldsymbol \gamma_{j2\cdot}$ is (non-uniform) DIF parameter for the second focal group,

and uniform DIF parameter
$$\boldsymbol \beta = (\boldsymbol \beta_{1},\boldsymbol \beta_{2},...,\boldsymbol \beta_{j},...,\boldsymbol \beta_{m})  (j=1,...,m)$$

$$\boldsymbol \beta_j =(\boldsymbol \beta_{j1\cdot},\boldsymbol \beta_{j2\cdot})^T
=\begin{pmatrix}
\beta_{j11} & \beta_{j12} & ...&\beta_{j1k} & ... & \beta_{j1,p-1}\\
\beta_{j21} & \beta_{j22} & ...&\beta_{j2k} & ... & \beta_{j2,p-1}\\
\end{pmatrix}(k=1,...,p-1),$$

where *p* is the number of categories in GRM, and each row of $\boldsymbol \beta_{j}$ is (uniform) DIF parameter for a focal group, i.e. $\boldsymbol \beta_{j1\cdot}$ is (uniform) DIF parameter for the first focal group, and $\boldsymbol \beta_{j2\cdot}$ is (uniform) DIF parameter for the second focal group.


If an item does not have DIF, then $\boldsymbol \Gamma=\boldsymbol 0$ and $\boldsymbol \beta=\boldsymbol 0$. If an item has uniform DIF, then $\boldsymbol \Gamma=\boldsymbol 0$.


The $N*m$ response matrix is

$$\textbf U=\begin{pmatrix}
u_{11} & u_{12} &... & u_{1j}& ... & u_{1m}\\
u_{21} & u_{22} &... & u_{2j}&... & u_{2m}\\
\vdots & \vdots & \ddots&\vdots & \ddots& \vdots\\
u_{i1} & u_{i2} &... & u_{ij}&... & u_{im}\\
\vdots & \vdots & \ddots&\vdots & \ddots& \vdots\\
u_{N1} & u_{N2} & ... & u_{Nj}&... & u_{Nm}\\
\end{pmatrix}(i=1,...,N; j=1,2,...,m).$$


A dummy variable to indicate whether examinee *i* gets score *k* for the item *j* $$x_{ijk} =\begin{cases} 1, & \mbox{if } u_{ij}=k \\ 0, & \mbox{if } u_{ij} \neq k \end{cases}.$$

$\boldsymbol y_i$ is the group indicator. $\boldsymbol y_i=(0,0)$ stands for the reference group, $\boldsymbol y_i=(1,0)$ stands for the rfirst focal group and $\boldsymbol y_i=(0,1)$ stands for the second focal group. The sample size of the reference group, the first focal group and the second focal group are denoted by $N_1$, $N_2$, $N_3$, respectively. We have the total sample size $N=N_1+N_2+N_3$. We have 

$$\boldsymbol Y=\begin{pmatrix}
\boldsymbol y_1 \\
\boldsymbol y_2 \\
\vdots \\
\boldsymbol y_{N_1}\\
\boldsymbol y_{N_1+1} \\
\boldsymbol y_{N_1+2} \\
\vdots \\
\boldsymbol y_{N_1+N_2} \\
\boldsymbol y_{N_1+N_2+1} \\
\boldsymbol y_{N_1+N_2+2}\\
\vdots \\
\boldsymbol y_{N_1+N_2+N_3}\\
\end{pmatrix}=\begin{pmatrix}
0 & 0 \\
0 & 0 \\
\vdots & \vdots \\
0 & 0 \\
1 & 0 \\
1 & 0 \\
\vdots & \vdots \\
1 & 0 \\
0 & 1 \\
0 & 1 \\
\vdots & \vdots \\
0 & 1 \\
\end{pmatrix}.$$


Suppose the prior distribution of $\boldsymbol \theta_i$ in group *y* is multivariate normal distribution with mean vector of $\boldsymbol\mu_y$ and covariance matrix of $\boldsymbol\Sigma_y$.
The prior density of $\boldsymbol \theta_i$ is
$$f(\boldsymbol \theta_{i} \mid \boldsymbol\mu_y,\boldsymbol\Sigma_y)=(2\pi)^{-\frac{p}{2}}|\boldsymbol\Sigma_y|^{-\frac{1}{2}}e^{-0.5(\boldsymbol\theta_i-\boldsymbol\mu_y)^T|\boldsymbol\Sigma_y|^{-1}(\boldsymbol\theta_i-\boldsymbol\mu_y)}.$$


If *i* is in 1,...,$N_1$, then $y_i=(0,0)$. $\boldsymbol\mu_{y}=\boldsymbol\mu_1$ and $\boldsymbol \Sigma_y=\boldsymbol \Sigma_1$, $\boldsymbol \theta_i\sim N(\boldsymbol\mu_1,\boldsymbol \Sigma_1)$.

If *i* is in $N_1+1$,...,$N_1+N_2$, then $y_i=(1,0)$. $\boldsymbol\mu_{y}=\boldsymbol\mu_2$ and $\boldsymbol \Sigma_y=\boldsymbol \Sigma_2$, $\boldsymbol \theta_i\sim N(\boldsymbol\mu_2,\boldsymbol \Sigma_2)$.

If *i* is in $N_1+N_2+1$,...,$N_1+N_2+N_3$, then $y_i=(0,1)$. $\boldsymbol\mu_{y}=\boldsymbol\mu_3$ and $\boldsymbol \Sigma_y=\boldsymbol \Sigma_3$, $\boldsymbol \theta_i\sim N(\boldsymbol\mu_3,\boldsymbol \Sigma_3)$.

We have $\boldsymbol\mu_1=\boldsymbol 0$ and all elements on the diagonal of $\boldsymbol \Sigma_1$ are 1
for the reference group. Then with some anchor items, the trait parameters for focal groups, i.e. $\boldsymbol\mu_2$, $\boldsymbol\mu_3$, $\boldsymbol \Sigma_2$ and $\boldsymbol \Sigma_3$, can be freely estimated.


Denote $G_0$ as the number of points we evenly take from each coordinate dimension. Then $G=G_0^q$ quadrature samples (same for all examinees) are denoted by $\boldsymbol q=(\boldsymbol q_{1},\boldsymbol q_{2},...,\boldsymbol q_{g},...,\boldsymbol q_{G})^T(g=1,...,G)$, and $\boldsymbol q_{g}$ = ($q_{g1}$,$q_{g2}$,...,$q_{gr}$,...,$q_{gq}$$)$ (r=1,2,...,q). At iteration *t*, we calculate $f(\boldsymbol q_{g} \mid \boldsymbol \mu^{(t-1)}_y,\boldsymbol\Sigma^{(t-1)}_y)$ for each group *y*, where $\boldsymbol \mu^{(t-1)}_y$ and $\boldsymbol\Sigma^{(t-1)}_y$ are the estimated trait parameters from last iteration.


For an examinee *i* in the reference group (group 1), we have $y=1$ and

$$P_{ijk\mid q_{g}}^*=P_{jyk\mid q_{g}}^* =P_{j1k\mid q_{g}}^*= \frac{1}{1+e^{-({\textbf a_j}{\boldsymbol q_g}+d_k)}} $$
$$(i=1,...,N_1;j=1,2,...,m; k=1,2,...,p-1;g=1,...,G).$$

For an examinee *i* in the first focal group (group 2), $y=2$ and

$$P_{ijk\mid q_{g}}^*=P_{jyk\mid q_{g}}^* =P_{j2k\mid q_{g}}^*= \frac{1}{1+e^{-({\textbf a_j}{\boldsymbol q_g}+d_k+\boldsymbol\gamma_{j1\cdot}\boldsymbol q_{g}+\boldsymbol\beta_{j1k})}}$$

$$(i=N_1+1,...,N_1+N_2;j=1,2,...,m; k=1,2,...,p-1;g=1,...,G).$$

For the second focal group (group 3), $y=3$ and

$$P_{ijk\mid q_{g}}^*=P_{jyk\mid q_{g}}^* =P_{j3k\mid q_{g}}^*= \frac{1}{1+e^{-({\textbf a_j}{\boldsymbol q_g}+d_k+\boldsymbol\gamma_{j2\cdot}\boldsymbol q_g+\boldsymbol\beta_{j2k})}}.$$

$$(i=N_1+N_2+1,...,N_1+N_2+N3;j=1,2,...,m; k=1,2,...,p-1;g=1,...,G).$$


$$P_{jyk\mid q_{g}} = P_{j,y,{k-1}\mid q_{g}}^* - P_{jyk\mid q_{g}}^*$$

# Model Identifiability Constraint

Some constraints on the item parameters are required for model identification. Here, for each dimension, we set one anchor item which we know its DIF parameters ($\Gamma$ and $\beta$) are zero for all groups.

For instance, if we have two ability dimensions (q=2), test length $m=20$, and the each factor is loaded on 10 items, then the simple structure discrimination parameter matrix will take the form 

$$\textbf A = (\textbf a_{1},...,\textbf a_{m})^T=\begin{pmatrix}
a_{11} & 0\\
0 & a_{22}\\
a_{31} & 0 \\
a_{41} & 0\\
a_{51} & 0\\
. & . \\
. & . \\
. & . \\
a_{10,1} & 0\\
a_{11,1} & 0\\
0 & a_{12,2}\\
0 & a_{13,2}\\
. & . \\
. & . \\
. & . \\
0 & a_{19,2}\\
0 & a_{20,2}\\
\end{pmatrix},$$

and the DIF parameters are

$$\boldsymbol \Gamma = (\boldsymbol 0,\boldsymbol 0,\boldsymbol \Gamma_{3},...,\boldsymbol \Gamma_{m})$$

and

$$\boldsymbol \beta = (\boldsymbol 0, \boldsymbol 0, \boldsymbol \beta_{3},...,\boldsymbol \beta_{m}).$$

We further assume the reference group has mean zero and variance one and only estimate its correlation, and the means and all the elements in covariance matrices of two focal groups can be freely estimated.

# Uniform DIF Detection via LASSO

As mentioned before, if an item has uniform DIF, then $\boldsymbol \Gamma=\boldsymbol 0$. The DIF parameter we are estimating is only $\boldsymbol \beta = (\boldsymbol 0, ...,\boldsymbol 0, \boldsymbol \beta_{q+1},...,\boldsymbol \beta_{m})$.

## E step

For an examinee with ability $\boldsymbol \theta_i$ the conditional likelihood of observing $\boldsymbol u_i$ is

\begin{equation}
\begin{aligned}
&L(\textbf A,\textbf D, \boldsymbol \beta, \boldsymbol \theta_i \mid \textbf y,\boldsymbol u_i)= \prod_{j=1}^m \prod_{k=1}^p P_{jk}(\boldsymbol \theta_i)^{x_{ijk}}.
\end{aligned}
\end{equation}

With the assumption of prior distribution of latent trait, the joint likelihood of $\boldsymbol u_i$ and $\boldsymbol \theta_i$ is

\begin{equation}
\begin{aligned}
&L(\textbf A,\textbf D, \boldsymbol \beta ,\boldsymbol \mu_y, \boldsymbol \Sigma_y \mid \textbf y,\textbf u_i, \boldsymbol \theta_i) = L(\textbf A,\textbf D,  \boldsymbol \beta, \boldsymbol \theta_i \mid \textbf y, \textbf u_i) f(\boldsymbol \mu_y, \boldsymbol \Sigma_y \mid \boldsymbol \theta_{i} )\\
&= \prod_{j=1}^m \prod_{k=1}^p P_{jk}(\boldsymbol \theta_i)^{x_{ijk}}(2\pi)^{-p/2}|\boldsymbol \Sigma_y|^{-1/2}\mathrm{exp}(-0.5(\boldsymbol \theta_i-\boldsymbol \mu_y)' \boldsymbol \Sigma_y^{-1}(\boldsymbol \theta_i-\boldsymbol \mu_y)).
\end{aligned}
\end{equation}

Therefore, the marginal likelihood of $\boldsymbol u_i$ is

\begin{equation}
\begin{aligned}
&m(\textbf A,\textbf D,\boldsymbol \beta ,\boldsymbol \mu_y, \boldsymbol \Sigma_y \mid \boldsymbol y,\boldsymbol u_i) = \int L(\textbf A,\textbf D, \boldsymbol \beta \mid \textbf y, \textbf u_i, \boldsymbol \theta_i) f(\boldsymbol \mu_y, \boldsymbol \Sigma_y \mid \boldsymbol \theta_{i} ) \partial \boldsymbol \theta_{i}
\end{aligned}
\end{equation}

Then

\begin{equation}
\begin{aligned}
&h(\boldsymbol \theta_{i}\mid \boldsymbol u_i,\boldsymbol y_i,\textbf A^{(t-1)},\textbf D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol\mu^{(t-1)}_y,\Sigma^{(t-1)}_y)=\frac{L(\textbf A,\textbf D, \boldsymbol \beta \mid \textbf y, \textbf u_i, \boldsymbol \theta_i) f(\boldsymbol \mu_y, \boldsymbol \Sigma_y \mid \boldsymbol \theta_{i})}{m(\textbf A,\textbf D, \boldsymbol \beta ,\boldsymbol \mu_y, \boldsymbol \Sigma_y \mid \boldsymbol y,\boldsymbol u_i)}
\end{aligned}
\end{equation}

is the posterior density of $\boldsymbol \theta_i$ given the estimation of $\textbf A$, $\textbf D$,  $\boldsymbol \beta$ and $\Sigma$ at the iteration *t*. 


The expected complete data log-likelihood with respect to the posterior distribution of $\boldsymbol\theta$

\begin{equation}
\begin{aligned}
& E[log\lbrace L(\boldsymbol A,\boldsymbol D, \boldsymbol \beta ,\boldsymbol \mu, \boldsymbol \Sigma \mid \boldsymbol Y,\boldsymbol U, \boldsymbol \Theta)  \rbrace|\boldsymbol A^{(t-1)},\boldsymbol D^{(t-1)}, \boldsymbol \beta^{(t-1)} ,\textbf Y,\textbf U, \boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)}] \\
& = \sum_i^N \lbrace \int \mathrm{log} L(\boldsymbol A,\boldsymbol D, \boldsymbol \beta \mid \boldsymbol y,\boldsymbol u_i, \boldsymbol \theta_i) h(\boldsymbol \theta_i|\boldsymbol y_i,\boldsymbol u_i,\boldsymbol A^{(t-1)},\boldsymbol D^{(t-1)} , \boldsymbol \beta^{(t-1)}, \boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)}) \partial  \boldsymbol \theta_i\\
&+\int \mathrm{log}f(\boldsymbol \mu_y,\boldsymbol \Sigma_y \mid\boldsymbol \theta_{i} ) h(\boldsymbol \theta_i|\boldsymbol u_i,\boldsymbol A^{(t-1)},\boldsymbol D^{(t-1)} , \boldsymbol \beta^{(t-1)},\boldsymbol \mu_y^{(t-1)}, \boldsymbol \Sigma_y^{(t-1)}) \partial  \boldsymbol \theta_i \rbrace
\end{aligned}
\end{equation}


At iteration *t*, applying Gauss-Hermite quadrature nodes and the integration above can be updated as

\begin{equation}
\begin{aligned}
&E[log L(\boldsymbol A,\boldsymbol D, \boldsymbol \beta ,\boldsymbol \mu, \boldsymbol \Sigma \mid \boldsymbol Y, \boldsymbol U)] \\
&=\sum_i^N \sum_g^G \mathrm{log} L(\boldsymbol A,\boldsymbol D, \boldsymbol \beta  \mid \boldsymbol u_i, \boldsymbol q_{g})\frac{L(\boldsymbol q_{g} \mid \boldsymbol u_i,\boldsymbol A^{(t-1)},\boldsymbol D^{(t-1)},\boldsymbol \beta^{(t-1)},\boldsymbol Y, \boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}{\sum_g^G L(\boldsymbol q_{g} \mid \boldsymbol u_i,\boldsymbol A^{(t-1)},\boldsymbol D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol Y,\boldsymbol \mu^{(t-1)},  \boldsymbol \Sigma^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}\\
& +\sum_i^N \sum_g^G \mathrm{log} f(\boldsymbol \mu, \boldsymbol \Sigma \mid\boldsymbol q_{g} ) \frac{L(\boldsymbol q_{g} \mid \boldsymbol u_i,\boldsymbol A^{(t-1)},\boldsymbol D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol Y, \boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}{\sum_g^G L(\boldsymbol q_{g} \mid \boldsymbol u_i,\boldsymbol A^{(t-1)},\boldsymbol D^{(t-1)},\boldsymbol \beta^{(t-1)},\boldsymbol Y,\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}\\
&=\sum_i^N \sum_g^G \sum_j^m \sum_k^p x_{ijk}\mathrm{log} P_{ijk\mid q_{g}} \frac{L(\boldsymbol q_{g} \mid \boldsymbol u_i,\boldsymbol A^{(t-1)},\boldsymbol D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol Y, \boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}{\sum_g^G L(\boldsymbol q_{g} \mid \boldsymbol u_i,\boldsymbol A^{(t-1)},\boldsymbol D^{(t-1)},\boldsymbol \beta^{(t-1)},\boldsymbol Y, \boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}\\
&+\sum_i^N \sum_g^G \mathrm{log} f(\boldsymbol \mu, \boldsymbol \Sigma \mid\boldsymbol q_{g} ) \frac{L(\boldsymbol q_{g} \mid \boldsymbol u_i,\boldsymbol A^{(t-1)},\boldsymbol D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol Y, \boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}{\sum_g^G L(\boldsymbol q_{g} \mid \boldsymbol u_i,\boldsymbol A^{(t-1)},\boldsymbol D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol Y, \boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}
\end{aligned}
\end{equation}





Then we can define two artificial terms.

For the reference group, $y=1$. We have

$$n_{gy} =n_{g1} = \sum_{i=1}^{N_1} \frac{L(\boldsymbol q_{g} \mid \textbf y_i,\boldsymbol u_i,\textbf A^{(t-1)},\textbf D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol \mu_1^{(t-1)}, \boldsymbol \Sigma_1^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}{\sum_g^G L(\boldsymbol q_{g} \mid \textbf y_i,\boldsymbol u_i,\textbf A^{(t-1)},\textbf D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol \mu_1^{(t-1)}, \boldsymbol \Sigma_1^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}, $$

and 

$$r_{gjyk} =r_{gj1k} =  \sum_{i=1}^{N_1}x_{ijk} \frac{L(\boldsymbol q_{g} \mid \textbf y_i,\boldsymbol u_i,\textbf A^{(t-1)},\textbf D^{(t-1)}, \boldsymbol \beta^{(t-1)}, \boldsymbol \mu_1^{(t-1)},\boldsymbol \Sigma_1^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}{\sum_g^G L(\boldsymbol q_{g} \mid \textbf y_i,\boldsymbol u_i,\textbf A^{(t-1)},\textbf D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol \mu_1^{(t-1)}, \boldsymbol \Sigma_1^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}.$$ 

For the first focal group, $y=2$. We have

$$n_{gy} =n_{g2} = \sum_{i=N_1+1}^{N_1+N_2} \frac{L(\boldsymbol q_{g} \mid \textbf y_i,\boldsymbol u_i,\textbf A^{(t-1)},\textbf D^{(t-1)},\boldsymbol \beta^{(t-1)},\boldsymbol \mu_2^{(t-1)}, \boldsymbol \Sigma_2^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}{\sum_g^G L(\boldsymbol q_{g} \mid \textbf y_i,\boldsymbol u_i,\textbf A^{(t-1)},\textbf D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol \mu_2^{(t-1)}, \boldsymbol \Sigma_2^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}, $$

and

$$r_{gjyk} =r_{gj2k} =  \sum_{i=N_1+1}^{N_1+N_2}x_{ijk} \frac{L(\boldsymbol q_{g} \mid \textbf y_i,\boldsymbol u_i,\textbf A^{(t-1)},\textbf D^{(t-1)},\boldsymbol \beta^{(t-1)}, \boldsymbol \mu_2^{(t-1)},\boldsymbol \Sigma_2^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}{\sum_g^G L(\boldsymbol q_{g} \mid \textbf y_i,\boldsymbol u_i,\textbf A^{(t-1)},\textbf D^{(t-1)},\boldsymbol \beta^{(t-1)},\boldsymbol \mu_2^{(t-1)}, \boldsymbol \Sigma_2^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}.$$ 

For the second focal group, $y=3$. We have

$$n_{gy} =n_{g3} =\sum_{i=N_1+N_2+1}^{N_1+N_2+N3} \frac{L(\boldsymbol q_{g} \mid \textbf y_i,\boldsymbol u_i,\textbf A^{(t-1)},\textbf D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol \mu_3^{(t-1)}, \boldsymbol \Sigma_3^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}{\sum_g^G L(\boldsymbol q_{g} \mid\textbf y_i,\boldsymbol u_i,\textbf A^{(t-1)},\textbf D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol \mu_3^{(t-1)}, \boldsymbol \Sigma_3^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})} $$

and

$$r_{gjyk} =r_{gj3k} =  \sum_{i=N_1+N_2+1}^{N_1+N_2+N3}x_{ijk} \frac{L(\boldsymbol q_{g} \mid \textbf y_i,\boldsymbol u_i,\textbf A^{(t-1)},\textbf D^{(t-1)}, \boldsymbol \beta^{(t-1)}, \boldsymbol \mu_3^{(t-1)},\boldsymbol \Sigma_3^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}{\sum_g^G L(\boldsymbol q_{g} \mid \textbf y_i,\boldsymbol u_i,\textbf A^{(t-1)},\textbf D^{(t-1)}, \boldsymbol \beta^{(t-1)},\boldsymbol \mu_3^{(t-1)}, \boldsymbol \Sigma_3^{(t-1)})f(\boldsymbol \mu^{(t-1)}, \boldsymbol \Sigma^{(t-1)} \mid \boldsymbol q_{g})}.$$ 



$n_g=n_{g1}+n_{g2}+n_{g3}$ represents the expected number of examinees with the ability $\boldsymbol q_{g}$, and $r_{jgk}=r_{jgk1}+r_{jgk2}+r_{jgk3}$ is the expected number of examinees who get the score level $k$ on the item *j* with the ability $\boldsymbol q_g$.

\begin{equation}
\begin{aligned}
E[log\lbrace L(\textbf A,\textbf D, \boldsymbol \beta, \boldsymbol \mu, \boldsymbol \Sigma \mid \textbf Y, \textbf U, \boldsymbol \Theta)] &= \sum_g^G \sum_j^m \sum_y^3 \sum_k^p  (r_{gjyk}  \mathrm{log} P_{jyk\mid q_{g}})+ \sum_g^G \sum_y^3  n_{gy} \mathrm{log} f(\boldsymbol \mu_y, \boldsymbol \Sigma_y \mid\boldsymbol q_{g} ) 
\end{aligned}
\end{equation}

In the EM problem, we want to maximize the above expectation at the iteration *t*. Denote this unpenalized expectation as $\log M$.

For each item *j*, we define 

\begin{equation}
\begin{aligned}
\log M_j &= \sum_g^G \sum_y^3 \sum_k^p  (r_{jgky}  \mathrm{log} P_{jky\mid q_{g}})+ \sum_g^G \sum_y^3 n_{gy} \mathrm{log} f(\boldsymbol \mu_y, \boldsymbol \Sigma_y \mid\boldsymbol q_{g}) 
\end{aligned}
\end{equation}

In our uniform DIF detection problem, the maximum likelihood method does not serve the purpose of DIF variable selection. We apply lasso and minimize the following objective function 

\begin{equation}
\begin{aligned}
-\log M + \eta \sum_j^m || \boldsymbol \beta_j ||_1 
\end{aligned}
\end{equation}

For each item, we minimize

\begin{equation}
\begin{aligned}
-\log M_j +  \eta || \boldsymbol \beta_j ||_1
\end{aligned}
\end{equation}


where $\eta$ is the lasso tuning parameter.

\begin{equation}
\begin{aligned}
(\hat{\boldsymbol A},\hat{\boldsymbol D},\hat{\boldsymbol \beta})=\mathrm{arg} \mathrm{min} \lbrace -\log M +  \eta || \boldsymbol \beta ||_1 \rbrace
\end{aligned}
\end{equation}

## M step


In our DIF detection problem, we assume the reference group has mean zero and variance one and only estimate the correlation, and the means and all the elements in covariance matrices of two focal groups can be freely estimated.

In quadrature method, at the iteration *t*, the first partial derivative with respect to $\mu$ is

\begin{equation}
\begin{aligned}
\frac{\partial \log M }{\partial \boldsymbol\mu_y}  &=\sum_g^G n_{gy} \frac{\partial \mathrm{log} f(\boldsymbol\mu_y,\boldsymbol \Sigma_y \mid\boldsymbol q_{g} )}{\partial \boldsymbol\mu_y}\\
&=\sum_g^G n_{gy}  \frac{\partial -\frac{1}{2}(\boldsymbol q_g-\boldsymbol \mu_y)^T \boldsymbol \Sigma_y^{-1}(\boldsymbol q_g-\boldsymbol \mu_y)}{\partial \boldsymbol\mu_y}\\
&=\sum_g^G n_{gy}  (\boldsymbol q_g-\boldsymbol \mu_y) \boldsymbol \Sigma_y^{-1}
\end{aligned}
\end{equation}

Set $\frac{\partial \log M }{\partial \boldsymbol\mu_y}=0$, and we know that $\sum_g^G n_{gy}=N_y$.

$\hat{\boldsymbol\mu}_y$ can be updated as

\begin{equation}
\begin{aligned}
\hat{\boldsymbol\mu}_2  =\frac{\sum_{g=1}^G n_{g2}\boldsymbol q_{g}}{N_2},
\end{aligned}
\end{equation}

and

\begin{equation}
\begin{aligned}
\hat{\boldsymbol\mu}_3  =\frac{\sum_{g=1}^G n_{g3}\boldsymbol q_{g}}{N_3}.
\end{aligned}
\end{equation}


The first partial derivative with respect to $\boldsymbol \Sigma$ is

\begin{equation}
\begin{aligned}
\frac{\partial \log M }{\partial \boldsymbol \Sigma_y}  &= \sum_g^G n_{gy} \frac{\partial \mathrm{log} f(\boldsymbol\mu_y,\boldsymbol \Sigma_y \mid\boldsymbol q_{g} )}{\partial \boldsymbol \Sigma_y}\\
&=\sum_g^G n_{gy} \frac{\partial (-\frac{q}{2}\mathrm{log}(2\pi)-\frac{1}{2}\mathrm{log}|\boldsymbol \Sigma_y|-\frac{1}{2}(\boldsymbol q_g-\boldsymbol \mu_y)^T \boldsymbol \Sigma_y^{-1}(\boldsymbol q_g-\boldsymbol \mu_y))}{\partial \boldsymbol \Sigma_y}\\
&=\sum_g^G n_{gy}[-\frac{1}{2}\Sigma_y^{-1}+\frac{1}{2}\Sigma_y^{-1}(\boldsymbol q_g-\boldsymbol \mu_y)(\boldsymbol q_g-\boldsymbol \mu_y)^T \Sigma_y^{-1}]
\end{aligned}
\end{equation}

Set $\frac{\partial \log M }{\partial \boldsymbol\mu_y}=0$, and use the fact that $\sum_g^G n_{gy}=N_y$.

$\hat{\boldsymbol\Sigma}_y$ can be updated as

\begin{equation}
\begin{aligned}
\hat{\boldsymbol\Sigma}_1=\frac{\sum_{g=1}^G n_{g1}\boldsymbol q_{g} \boldsymbol q_{g}'}{N_1},
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\hat{\boldsymbol\Sigma}_2=\frac{\sum_{g=1}^G n_{g2}(\boldsymbol q_{g}-\hat{\boldsymbol\mu}_2) (\boldsymbol q_{g}-\hat{\boldsymbol\mu}_2)'}{N_2},
\end{aligned}
\end{equation}

and

\begin{equation}
\begin{aligned}
\hat{\boldsymbol\Sigma}_3=\frac{\sum_{g=1}^G n_{g3}(\boldsymbol q_{g}-\hat{\boldsymbol\mu}_3) (\boldsymbol q_{g}-\hat{\boldsymbol\mu}_3)'}{N_3}.
\end{aligned}
\end{equation}

To standardize the covariance matrix, we calculate standardized quadrature points for the later steps.

\begin{equation}
\begin{aligned}
\boldsymbol q_{g}^*=\frac{q_{g}}{\sqrt{\mathrm{diag}\hat{\boldsymbol\Sigma_1}}}.
\end{aligned}
\end{equation}

Then we do the following transformation on mean vector and covariance matrices for three groups.

\begin{equation}
\begin{aligned}
\hat{\boldsymbol\Sigma}_1^*=\frac{\sum_{g=1}^G n_{g1}\boldsymbol q_{g}^* \boldsymbol q_{g}^{*'}}{N_1},
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\hat{\boldsymbol\Sigma}_2^*=\frac{\sum_{g=1}^G n_{g2}(\boldsymbol q_{g}^*-\hat{\boldsymbol\mu}_2) (\boldsymbol q_{g}^*-\hat{\boldsymbol\mu}_2)'}{N_2},
\end{aligned}
\end{equation}

and

\begin{equation}
\begin{aligned}
\hat{\boldsymbol\Sigma}_3^*=\frac{\sum_{g=1}^G n_{g3}(\boldsymbol q_{g}^*-\hat{\boldsymbol\mu}_3) (\boldsymbol q_{g}^*-\hat{\boldsymbol\mu}_3)'}{N_3}.
\end{aligned}
\end{equation}


the first partial derivative with respect to $a_{jr}$ is

\begin{equation}
\begin{aligned}
\frac{\partial \log M}{\partial a_{jr}} =  \sum_{g=1}^{G} \sum_{y}^3 \sum_{k=1}^p (\frac{r_{gjyk} q_{gr}}{P_{jky\mid q_{g}}}(\omega_{j,y,k-1}-\omega_{jyk}))
\end{aligned}
\end{equation}


where $\omega_{jyk} = P_{jyk\mid q_{g}}^*-(P_{jky\mid q_{g}}^*)^2$.

Similarly, we have the first partial derivative with respect to $d_{jk}$

\begin{equation}
\begin{aligned}
\frac{\partial \log M}{\partial d_{jk}} =\sum_g^G \sum_y^3 \omega_{jky} (\frac{r_{gj,(k+1),y}}{P_{jy,(k+1)\mid q_{g}}}-\frac{r_{gjyk}}{P_{jyk\mid q_{g}}})
\end{aligned}
\end{equation}

where $\omega_{jyk} = P_{jyk\mid q_{g}}^*-(P_{jyk\mid q_{g}}^*)^2$,

and the first partial derivative with respect to $\beta_{jyk}$, where y=(2,3), is

\begin{equation}
\begin{aligned}
\frac{\partial \log M}{\partial \beta_{jyk}} = \sum_g^G \omega_{jyk} (\frac{r_{gjy,(k+1)}}{P_{jy,(k+1)\mid q_{g}}}-\frac{r_{gjyk}}{P_{jyk\mid q_{g}}})
\end{aligned}
\end{equation}

where $\omega_{jyk} = P_{jyk\mid q_{g}}^*-(P_{jyk\mid q_{g}}^*)^2$.



The second partial derivatives in the Hessian matrix are given by

\begin{equation}
\begin{aligned}
&\frac{\partial^2 \log M}{\partial a_{jr}^2}= \sum_{g=1}^{G} \sum_y^3 \sum_{k=1}^p -\frac{r_{gjyk} q_{gr}^2(P_{jy,(k-1)\mid q_{g}}^{*}Q_{jy,(k-1)\mid q_{g}}^{*}-P_{jyk\mid q_{g}}^{*}Q_{jyk\mid q_{g}}^{*})^2}{P_{jyk\mid q_{g}}^2}\\
&=\sum_{g=1}^{G} \sum_y^3 \sum_{k=1}^p  -\frac{r_{gjyk} q_{gr}^2(\omega_{jy(k-1)}-\omega_{jyk})}{P_{jyk\mid q_{g}}^2}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
&\frac{\partial^2 \log M}{\partial d_{jk}^2}= \sum_y^3\sum_{g=1}^{G} -(\frac{r_{gjyk}}{P_{jyk\mid q_{g}}^2}+\frac{r_{gjy(k+1)}}{P_{jy(k+1)\mid q_{g}}^2})P_{jyk\mid q_{g}}^{*2}(1-P_{jyk\mid q_{g}}^{*})^2\\
&=\sum_y^3 \sum_{g=1}^{G} -(\frac{r_{gjyk}}{P_{jyk\mid q_{g}}^2}+\frac{r_{gjy(k+1)}}{P_{jy(k+1)\mid q_{g}}^2})\omega_{jyk}^2
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
&\frac{\partial^2 \log M}{\partial d_{jk}\partial d_{j,k+1}}= \sum_{g=1}^{G} \sum_y^3 \frac{r_{gjy(k+1)}}{P_{jy(k+1)\mid q_{g}}^2}(P_{jyk\mid q_{g}}^{*2}(1-P_{jyk\mid q_{g}}^{*})^2)(P_{jy(k+1)\mid q_{g}}^{*2}(1-P_{jy(k+1)\mid q_{g}}^{*})^2)\\
&= \sum_{g=1}^{G} \sum_y^3 \frac{r_{gjy(k+1)}}{P_{jy(k+1)\mid q_{g}}^2}\omega_{jyk}^2\omega_{jy(k+1)}^2\\
\end{aligned}
\end{equation}

and

\begin{equation}
\begin{aligned}
\frac{\partial^2 \log M}{\partial a_{jr}\partial d_{jk}}&= \sum_{g=1}^{G} \sum_y^3 P_{jyk}^{*}Q_{jyk}^{*}q_{gr}[\frac{r_{gjyk}}{P_{jyk\mid q_{g}}^2}(P_{jy(k-1)\mid q_{g}}^{*}Q_{jy(k-1)\mid q_{g}}^{*}-P_{jyk\mid q_{g}}^{*}Q_{jyk\mid q_{g}}^{*}) \\
&+\frac{r_{gjy(k+1)}}{P_{jy(k+1)\mid q_{g}}^2}(P_{jyk\mid q_{g}}^{*}Q_{jyk\mid q_{g}}^{*}-P_{jy(k+1)\mid q_{g}}^{*}Q_{jy(k+1)\mid q_{g}}^{*})]\\
&= \sum_{g=1}^{G} \sum_y^3 \omega_{jyk}q_{gr}[\frac{r_{gjyk}}{P_{jyk\mid q_{g}}^2}(\omega_{jy(k-1)}-\omega_{jyk}) +\frac{r_{gjy(k+1)}}{P_{jy(k+1)\mid q_{g}}^2}(\omega_{jyk}-\omega_{jy(k+1)})]
\end{aligned}
\end{equation}

where

$$Q_{jyk\mid q_{g}}^{*}=1-P_{jyk\mid q_{g}}^{*}.$$
$$\omega_{jyk} = P_{jyk\mid q_{g}}^{*}*Q_{jky\mid q_{g}}^{*}$$

\begin{equation}
\begin{aligned}
&\frac{\partial^2 \log M}{\partial \beta_{jyk}^2}= \frac{\partial^2 \log M}{\partial \beta_{jyk} \partial d_{jk}}=\sum_{g=1}^{G} -(\frac{r_{gjyk}}{P_{jyk\mid q_{g}}^2}+\frac{r_{gjy(k+1)}}{P_{jy(k+1)\mid q_{g}}^2})P_{jyk\mid q_{g}}^{*2}(1-P_{jyk\mid q_{g}}^{*})^2
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\frac{\partial^2 \log M}{\partial a_{jr}\partial \beta_{jyk}}= \sum_{g=1}^{G} \omega_{jyk}q_{gr}[\frac{r_{gjyk}}{P_{jyk\mid q_{g}}^2}(\omega_{jy(k-1)}-\omega_{jyk}) +\frac{r_{gjy(k+1)}}{P_{jy(k+1)\mid q_{g}}^2}(\omega_{jyk}-\omega_{jy(k+1)})]
\end{aligned}
\end{equation}

The expectation of the second partial derivatives in the Fisher scoring method are given by

\begin{equation}
\begin{aligned}
E(\frac{\partial^2 \log M}{\partial a_{jr}^2})=\sum_{g=1}^{G} \sum_y^3 \sum_{k=1}^p  -\frac{n_{gy} q_{gr}^2(\omega_{jy(k-1)}-\omega_{jyk})}{P_{jyk\mid q_{g}}},
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
E(\frac{\partial^2 \log M}{\partial d_{jk}^2})= \sum_{g=1}^{G} \sum_y^3 -n_{gy}(\frac{1}{P_{jyk\mid q_{g}}}+\frac{1}{P_{jy(k+1)\mid q_{g}}})\omega_{jyk}^2,
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
E(\frac{\partial^2 \log M}{\partial d_{jk}\partial d_{j,k+1}})=\sum_{g=1}^{G} \sum_y^3 \frac{n_{gy}}{P_{jy(k+1)\mid q_{g}}}\omega_{jyk}^2\omega_{jy(k+1)}^2,
\end{aligned}
\end{equation}

and

\begin{equation}
\begin{aligned}
E(\frac{\partial^2 \log M}{\partial a_{jr}\partial d_{jk}})= \sum_{g=1}^{G} \sum_y^3 n_{gy} \omega_{jyk}q_{gr}[\frac{1}{P_{jyk\mid q_{g}}}(\omega_{jy(k-1)}-\omega_{jyk}) +\frac{1}{P_{jy(k+1)\mid q_{g}}}(\omega_{jyk}-\omega_{jy(k+1)})].
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
E(\frac{\partial^2 \log M}{\partial \beta_{jyk}^2})= E(\frac{\partial^2 \log M}{\partial \beta_{jyk} \partial d_{jk}})=\sum_{g=1}^{G} -n_{gy}(\frac{1}{P_{jyk\mid q_{g}}}+\frac{1}{P_{jy(k+1)\mid q_{g}}})\omega_{jyk}^2.
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
E(\frac{\partial^2 \log M}{\partial a_{jr}\partial \beta_{jyk}})=\sum_{g=1}^{G} n_{gy} \omega_{jyk}q_{gr}[\frac{1}{P_{jyk\mid q_{g}}}(\omega_{jy(k-1)}-\omega_{jyk}) +\frac{1}{P_{jy(k+1)\mid q_{g}}}(\omega_{jyk}-\omega_{jy(k+1)})].
\end{aligned}
\end{equation}

### Cyclical coordinate descent

By Bazaraa, Sherali, and Shetty (2006), for a convex function *f*, a point $\bar\theta$ is a global minimizer of *f* if and only if $\partial f(\bar\theta)$, the subgradient of *f* at  $\bar\theta$, contains 0. Hence $\hat\theta_{\tau}$ is the global minimizer only when $\hat\theta_{\tau}=\mathrm{sign}(s) (|s|-\tau)_+$, where $(u)_+=u1(u>0).$ This is called the soft-threshold of *s* and $\tau$, and can be denoted by

\begin{equation}
\begin{aligned}
&\hat\theta_{\tau}=\mathrm{soft} (s,\tau)\equiv\mathrm{sign}(s) (|s|-\tau)_+ \\
&=\arg \min_{\theta \in \mathbb{R}} \lbrace0.5\theta^2-s\theta+\tau|\theta|\rbrace.
\end{aligned}
\end{equation}

Then, to minimize our objective function with respect to $\boldsymbol \beta$, we calculate a second-order Tylor approximation of the marginal log-likelihood at $\boldsymbol \beta^{(t-1)}$, and our lasso estimator in (13) can be updated by 

\begin{equation}
\begin{aligned}
\hat{\boldsymbol \beta}&=\mathrm{arg} \mathrm{min} \lbrace -\log M(\boldsymbol \beta) +  \eta || \boldsymbol \beta ||_1 \rbrace\\
&= \mathrm{arg} \mathrm{min} \lbrace -\log M(\boldsymbol \beta^{(t-1)})- \partial_{\beta} \log M(\boldsymbol \beta^{(t-1)})(\boldsymbol \beta-\boldsymbol \beta^{(t-1)})-\frac{\partial^2_{\beta} \log M(\boldsymbol \beta^{(t-1)})}{2}(\boldsymbol \beta-\boldsymbol \beta^{(t-1)})^2 +  \eta || \boldsymbol \beta ||_1 \rbrace\\
&=-\frac{\mathrm{soft}(\partial_{\boldsymbol \beta}\log M-\boldsymbol \beta_j^{(t-1)}*\partial^2_{\boldsymbol \beta}\log M,\eta)}{\partial^2_{\boldsymbol \beta}\log M}
\end{aligned}
\end{equation}




We run an EM and cyclical coordinate descent algorithm given by following.

\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\boldsymbol A_0$,$\boldsymbol D_0$,$\boldsymbol \beta_0$,$\boldsymbol \mu_0$,$\boldsymbol \Sigma_0$,U,$\eta$,$\varepsilon_1$,$\varepsilon_2$}
\Output{$\hat{\boldsymbol A}$,$\hat{\boldsymbol D}$,$\hat{\boldsymbol \beta}$,$\hat{\boldsymbol \mu}$,$\hat{\boldsymbol \Sigma}$}
\BlankLine
set $t_1=1$, $\delta^{(t_1-1)}=$ any value greater than $\varepsilon_1$\;
\While{$\delta_1^{(t_1-1)}>\varepsilon_1$}{
    Calculate $n_{gy}$ and $r_{gjyk}$\;
    Update $\boldsymbol \mu^{(t_1)}$ and $\boldsymbol \Sigma^{(t_1)}$\;
    \For {j=1,...m}{
    set $t_2=1$, $\delta^{(t_2-1)}=$ any value greater than $\varepsilon_2$\;
    \While{$\delta_2^{(t_2-1)}>\varepsilon_2$}{
        Calculate $P_{jyk\mid q_{g}}^{*},Q_{jyk\mid q_{g}}^{*}$\;
        ${a}_{jr}^{(t_2)}=  a_{jr}^{(t_2-1)}-\frac{\partial_{ a_{jr}}\log M}{\partial^2_{ a_{jr}}\log M}$\;
        ${d}_{jk}^{(t_2)}=  d_{jk}^{(t_2-1)}-\frac{\partial_{ d_{jk}}\log M}{\partial^2_{ d_{jk}}\log M}$\;
        $\beta^{(t_2)}_{jyk}=-\frac{\mathrm{soft}(\partial_{\beta_{jyk}}\log M- \beta_{jyk}^{(t_2-1)}*\partial^2_{ \beta_{jyk}}\log M,\eta)}{\partial^2_{\beta_{jyk}}\log M}$\;
        $\delta_2^{(t_2)}=||\boldsymbol A_j^{(t_2)}-\boldsymbol A_{j}^{(t_2-1)}||+||\boldsymbol D_j^{(t_2)}-\boldsymbol D_{j}^{(t_2-1)}||+||\boldsymbol \beta_j^{(t_2)}-\boldsymbol \beta_{j}^{(t_2-1)}||$\;
        $t_2=t_2+1$\; 
    }
    ${a}_{jr}^{(t_1)*}=  {a}_{jr}^{(t_1)}*\sqrt{\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})}$\;
    }
   $\delta_1^{(t_1)}=||\boldsymbol A^{(t_1)}-\boldsymbol A^{(t_1-1)}||+||\boldsymbol D^{(t_1)}-\boldsymbol D^{(t_1-1)}||+||\boldsymbol \beta^{(t_1)}-\boldsymbol \beta^{(t_1-1)}||$\;
   $t_1=t_1+1$\; 
}
\caption{Uniform DIF Detection via LASSO}
\end{algorithm} 

Note that $\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})$ is the *r*th element on the diagonal of the estimated covariance matrix of the reference group $\hat{\boldsymbol {\boldsymbol\Sigma}_1}$.

$\varepsilon_1=10^{-3}$ and $\varepsilon_2=10^{-7}$.

## Tuning Parameter Selectiion

The Bayesian information criterion (BIC) is applied for tuning parameter selection. The BIC can be calculated by
$$BIC_{\hat{\textbf A},\hat{\textbf D},\hat{\boldsymbol\beta}}=-2\max_{\hat{\textbf A},\hat{\textbf D},\hat{\boldsymbol\beta}}\log M+ \|\hat{\boldsymbol\beta}\|_0 \log N, \tag{11}$$
where N is the sample size. The first term control the bias of the estimator, and the second term penalize the complexity of the model. We want to choose a tuning parameter $\eta$ which gives us the smallest BIC value.

## Simulation

### Simulation 1

$\emph Sample$ $\emph Size.$ The total sample size is $N=1500$, and the group sample sizes are $N_1=N_2=N_3=500$.

$\emph Test$ $\emph Length.$ $m=20.$ Simple structure. 10 items per dimension.

$\emph Proportion \ of \ DIF.$ 4 items with DIF. 2 DIF items per dimension.

$\emph Magnitude \ of \ DIF.$ The first focal group with larger difficulty parameters (+0.5) on the 4 DIF items. 

The second focal group with much larger difficulty parameters (+1) on the 4 DIF items.

$\emph Generated \ parameters.$

$a_{j1}\sim U(1.5,2.5), j=1,...,10$

$a_{j2}\sim U(1.5,2.5), j=11,...,20$

$d_{1}\sim N(0,1)$

$$\boldsymbol A=\begin{pmatrix}
2.17&0\\
0& 2.46\\
2.41& 0\\
2.45& 0\\
2.34& 0\\
1.84& 0\\
1.85& 0\\
1.92& 0\\
1.94& 0\\
1.90& 0\\
1.92& 0\\
0& 2.43\\
0& 1.82\\
0& 2.22\\
0& 1.93\\
0& 1.88\\
0& 1.84\\
0& 2.12\\
0& 2.42\\
0& 2.15\\
\end{pmatrix},$$

$$\boldsymbol D=\begin{pmatrix}
0.03\\
-1.28\\
0.58\\
-2.06\\
0.12\\
3.25\\
-0.41\\
-0.51\\
0.89\\
1.33\\
0.85\\
0.82\\
-0.37\\
-0.99\\
-0.27\\
0.19\\
1.73\\
0.05\\
-1.86\\
-0.63\\
\end{pmatrix},$$

$$\boldsymbol\beta=\begin{pmatrix}
0 & 0\\
0 & 0 \\
0 & 0 \\
0.5 & 1 \\
0.5 & 1 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0.5 & 1 \\
0.5 & 1 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
\end{pmatrix},$$

wABC

item | 4 | 5 | 12 | 13 |
------|------|------|------|------|
Focal 1| 0.886 | 1.052 | 0.998 | 1.275 |
Focal 2| 1.857 | 2.068 | 1.924 | 2.537 |


The first two items are used as anchor items in estimation.

No impact. $\theta_i\sim N(\begin{pmatrix}0\\0\\\end{pmatrix}, \begin{pmatrix}1&0.85\\0.85&1\\\end{pmatrix})$

$\emph Tuning \ parameters.$ Tuning parameters $\eta$ are chosen from a sequence starting from 21 to 51 with increment 3. The range [21,51] was chosen because in all simulation studies I’ve run so far, the true model will be selected by the tuning parameter in this range for 100%.

Results of 50 Replications 

$\emph Table \ 1. \ Type \ I \ error \ and \ Power \ of \ regularization \ method$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.985 | 0.34 | 0.985 |
Type I| 0.047 | 0.024 | 0.024 |

Omnibus DIF in regularization is defined as if at least one focal group showd DIF on an item, then that item is flagged as DIF. But in mirt package, omnibus DIf is testing $H_0$: the item has no DIF v.s. $H_\alpha:$ all focal groups have DIF.

$\emph Table \ 2. \ Type \ I \ error \ and \ Power \ of \ mirt \ LRT \ (0.05 \ significance \ level)$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.86 | 0.23 | 0.91 |
Type I| 0.00714 | 0.0085 | 0.002857 |


mirt wald

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.2031 | 0.2031 | 0.9531 |
Type I| 0.0045 | 0.0045 | 0.0134 |

$\emph Table \ 3. \  Item \ parameter \ estimates \ by \ regularization$

Item Parameters |$\boldsymbol a_1$| $\boldsymbol a_2$| $\boldsymbol d$| 
------------------ | -----|------|------|------|------|------|
Bias|-0.011145| -0.00684|-0.04356|
RMSE|0.1676937|0.1614306|0.1576307|

$\emph Table \ 4. \ Item \ parameter \ estimates \ by \ mirt \ LRT \ (0.05 \ significance \ level)$

Item Parameters |$\boldsymbol a_1$| $\boldsymbol a_2$| $\boldsymbol d$| 
------------------ | -----|------|------|------|------|------|
Bias|0.01262| 0.00636|-0.00532|
RMSE|0.17259|0.16686|0.15026|

Our regularization method has slightly better non-DIF item parameter estimates.

$\emph Table \ 5. \ Absolute \ bias \ for \ DIF \ magnitude \ recoveries \ for \ true \ DIF \ items$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Regularization (include false negative)| 0.1529 | 0.3779 | 0.2284 |
mirt LRT (include false negative)| 0.2222 | 0.4181 | 0.2181 |

$\emph Table \ 6. \ Trait \ Estimation$

Item Parameters |$\theta_1$| $\theta_2$| 
------------------ | -----|------|
Bias| 0.0365 | 0.0374|
RMSE| 0.3823682 | 0.3806336 |
SE | 0.3644813 | 0.3590251 |


```{r eval=FALSE, include=FALSE}
for (rep in 1:50){
  resp=responses[((rep-1)*N+1):((rep-1)*N+N1+N2+N3),]
  resp01=resp[1:(N1+N2),]
  resp02=rbind(resp[1:N1,],resp[(N1+N2+1):(N1+N2+N3),])
  s <- 'D1 = 1,3-11
          D2 = 2,12-20
          COV = D1*D2'
  #omnibus
  md.refit.r <- multipleGroup(resp, s, group = Group,SE=TRUE,invariance=c('free_means', 'free_var','slopes',colnames(resp)[-c(4,5,12,13)]))
  difrec.mirt.fn1.r[rep,2:3]= c(mean(abs((coef(md.refit.r,simplify=T)$G2$items[,3]-coef(md.refit.r,simplify=T)$G1$items[,3])[c(4,5,12,13)]-0.5)),mean(abs((coef(md.refit.r,simplify=T)$G3$items[,3]-coef(md.refit.r,simplify=T)$G1$items[,3])[c(4,5,12,13)]-1)))
  difrec.mirt.fn1.r[rep,1]=mean(c(difrec.mirt.fn1.r[rep,2],difrec.mirt.fn1.r[rep,3])) #omnibus DIF recovery in report (only need this)
  }

```
The results in Table 6 are the average of estimated DIF for false positive items. LRT by mirt performs better when type I error happens. The type I error is low, so the probability to have these bias is low.

### Simulation 2

Increase DIF proportion to 60% and keep everything else the same in simulation 1.

$\emph Sample$ $\emph Size.$ The total sample size is $N=1500$, and the group sample sizes are $N_1=N_2=N_3=500$.

$\emph Proportion \ of \ DIF.$ 12 items with DIF. 6 DIF items per dimension.

$$\boldsymbol\beta=\begin{pmatrix}
0 & 0\\
0 & 0 \\
0 & 0 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0 & 0 \\
0 & 0 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
\end{pmatrix},$$

wABC

item | 4 | 5 | 6 | 7 | 8 | 9 | 12 | 13 | 14 | 15 | 16 | 17 |
------|------|------|------|------|------|------|------|------|------|------|------|------|
Focal 1| 0.886 | 1.052 | 0.636 | 1.265 | 1.255 | 1.116 | 0.998 | 1.275 | 1.091 | 1.234 | 1.233 | 0.918 |
Focal 2| 1.857 | 2.067 | 1.385 | 2.521 | 2.513 | 2.117 | 1.923 | 2.537 | 2.218 | 2.448 | 2.399 | 1.684 | 

$\emph Table \ 7. \ Type \ I \ error \ and \ Power \ of \ regularization \ method$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.872 | 0.136 | 0.872 |
Type I| 0.135 | 0.085 | 0.053 |

Omnibus DIF is defined as if at least one focal group showd DIF on an item, then that item is flagged as DIF.

$\emph Table \ 8. \ Type \ I \ error \ and \ Power \ of \ mirt \ LRT \ (0.05 \ significance \ level)$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.963 | 0.395 | 0.987 |
Type I| 0.0175 | 0.0125 | 0.0175 |

### Simulation 3

Increase total sample size to 3000 and keep everything else the same in simulation 1.

$\emph Sample$ $\emph Size.$ The total sample size is $N=3000$, and the group sample sizes are $N_1=N_2=N_3=1000$.

$\emph Proportion \ of \ DIF.$ 4 items with DIF. 2 DIF items per dimension.

$\emph Table \ 9. \ Type \ I \ error \ and \ Power \ of \ regularization \ method$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 1 | 0.775 | 1 |
Type I| 0.051 | 0.027 | 0.028 |

Omnibus DIF is defined as if at least one focal group showd DIF on an item, then that item is flagged as DIF.

$\emph Table \ 10. \ Type \ I \ error \ and \ Power \ of \ mirt \ LRT \ (0.05 \ significance \ level)$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.995 | 0.63 | 0.995 |
Type I| 0.008 | 0.01 | 0.012 |


$\emph Table \ 11. \  Item \ parameter \ estimates \ by \ regularization$

Item Parameters |$\boldsymbol a_1$| $\boldsymbol a_2$| $\boldsymbol d$| 
------------------ | -----|------|------|------|------|------|
Bias|-0.01364| -0.00353|-0.0316|
RMSE|0.11486|0.11917|0.111221|

$\emph Table \ 12. \ Item \ parameter \ estimates \ by \ mirt \ LRT \ (0.05 \ significance \ level)$

Item Parameters |$\boldsymbol a_1$| $\boldsymbol a_2$| $\boldsymbol d$| 
------------------ | -----|------|------|------|------|------|
Bias|0.00612| 0.01542|0.009|
RMSE|0.11613|0.12292|0.0953|


$\emph Table \ 13. \ Absolute \ bias \ for \ DIF \ magnitude \ recoveries \ that \ were \ true \ DIF$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Regularization (include false negative)| 0.1111 | 0.1884 | 0.1451 |
mirt LRT (include false negative)| 0.1145 | 0.2557 | 0.1155 |

### Simulation 4

Increase total sample size to 3000 and keep everything else the same in simulation 2.

$\emph Sample$ $\emph Size.$ The total sample size is $N=3000$, and the group sample sizes are $N_1=N_2=N_3=1000$.

$\emph Proportion \ of \ DIF.$ 12 items with DIF. 6 DIF items per dimension.

$\emph Table \ 14. \ Type \ I \ error \ and \ Power \ of \ regularization \ method$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.998 | 0.58 | 0.998 |
Type I| 0.05 | 0.04 | 0.01 |


$\emph Table \ 15. \ Type \ I \ error \ and \ Power \ of \ mirt \ LRT \ (0.05 \ significance \ level)$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.996 | 0.772 | 1 |
Type I| 0.02 | 0.02 | 0.0225 |


$\emph Table \ 16. \  Item \ parameter \ estimates \ by \ regularization$

Item Parameters |$\boldsymbol a_1$| $\boldsymbol a_2$| $\boldsymbol d$| 
------------------ | -----|------|------|------|------|------|
Bias|-0.0316| 0.0244| 0.0192|
RMSE|0.118|0.127|0.131|

$\emph Table \ 17. \ Item \ parameter \ estimates \ by \ mirt \ LRT \ (0.05 \ significance \ level)$

Item Parameters |$\boldsymbol a_1$| $\boldsymbol a_2$| $\boldsymbol d$| 
------------------ | -----|------|------|------|------|------|
Bias|0.0075| 0.0197 | 0.0093 |
RMSE|0.1188|0.1269|0.1062|


$\emph Table \ 18. \ Absolute \ bias \ for \ DIF \ magnitude \ recoveries \ that \ were \ true \ DIF$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Regularization (include false negative)| 0.216 | 0.2663 | 0.167 |
mirt LRT (include false negative)| 0.1843 | 0.3556 | 0.1155 |


# Non-uniform DIF Detection via LASSO 


When the items have non-uniform DIF on slope only, i.e., there is no DIF on the intercepts, the DIF parameter we are estimating is $\boldsymbol \Gamma = (\boldsymbol 0,..., \boldsymbol 0, \boldsymbol \Gamma_{q+1},...,\boldsymbol \Gamma_{m})$.

## E step

For the expectation step, we can use the result in Section 3.1 only replacing the DIF parameter $\boldsymbol \beta$ by $\boldsymbol \Gamma$, and minimize the following objective function 

\begin{equation}
\begin{aligned}
-\log M + \eta \sum_j^m ||\boldsymbol \Gamma_j||_1
\end{aligned}
\end{equation}

For each item, we minimize

\begin{equation}
\begin{aligned}
-\log M_j +  \eta ||\boldsymbol \Gamma_j||_1
\end{aligned}
\end{equation}


where $\eta$ is the lasso tuning parameter.

\begin{equation}
\begin{aligned}
(\hat{\boldsymbol A},\hat{\boldsymbol D},\hat{\boldsymbol \Gamma})=\mathrm{arg} \mathrm{min} \lbrace -\log M +  \eta || \boldsymbol \Gamma ||_1 \rbrace
\end{aligned}
\end{equation}

## M step

Again, we assume the reference group has mean zero and variance one and only estimate its correlations. The means and all elements in the covariance matrices of two focal groups can be freely estimated.

$\hat{\boldsymbol\mu}_2,\hat{\boldsymbol\mu}_3,\hat{\boldsymbol\Sigma}_1^*,\hat{\boldsymbol\Sigma}_2^*,\hat{\boldsymbol\Sigma}_3^*$ are same in (15), (16), (22), (23) and (24).

The first partial derivative with respect to $a_{jr}$ and $d_{jk}$ are same as in (25) and (26).


the first partial derivative with respect to $\gamma_{jry}$, where y=(2,3), is

\begin{equation}
\begin{aligned}
\frac{\partial \log M}{\partial \gamma_{jyr}} &= \sum_g^G \sum_k^p \frac{r_{gjyk} q_{gr} [P_{jy(k-1)\mid q_{g}}^*(1-P_{jy(k-1) \mid q_{g}}^*)-P_{jyk\mid q_{g}}^*(1-P_{jyk\mid q_{g}}^*)] }{P_{jyk\mid q_{g}}}\\
&= \sum_{g}^{G} \sum_{k}^p  (\frac{r_{gjyk} q_{gr}}{P_{jyk\mid q_{g}}}(\omega_{jy(k-1)}-\omega_{jyk}))
\end{aligned}
\end{equation}

where $\omega_{jyk} = P_{jky\mid q_{g}}^*-(P_{jyk\mid q_{g}}^*)^2$.



The second partial derivatives in the Hessian matrix $\frac{\partial^2 \log M}{\partial a_{jr}^2}$, $\frac{\partial^2 \log M}{\partial d_{jk}^2}$, $\frac{\partial^2 \log M}{\partial d_{jk}\partial d_{j,k+1}}$ and $\frac{\partial^2 \log M}{\partial a_{jr}\partial d_{jk}}$ are given by (28)-(31) and their expectations are (34)-(37).



\begin{equation}
\begin{aligned}
&\frac{\partial^2 \log M}{\partial \gamma_{jyr}^2}= \frac{\partial^2 \log M}{\partial \gamma_{jyr}\partial a_{jr}}=\sum_{g=1}^{G} \sum_{k=1}^p  -\frac{r_{gjyk} q_{gr}^2(\omega_{jy(k-1)}-\omega_{jyk})}{P_{jyk\mid q_{g}}^2}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\frac{\partial^2 \log M}{\partial \gamma_{jyr}\partial d_{jk}}= \sum_{g=1}^{G} \omega_{jky}q_{gr}[\frac{r_{gjyk}}{P_{jyk\mid q_{g}}^2}(\omega_{jy(k-1)}-\omega_{jyk}) +\frac{r_{gjy(k+1)}}{P_{jy(k+1)\mid q_{g}}^2}(\omega_{jyk}-\omega_{jy(k+1)})]
\end{aligned}
\end{equation}

The expectation of the second partial derivatives in the Fisher scoring method are given by

\begin{equation}
\begin{aligned}
E(\frac{\partial^2 \log M}{\partial \gamma_{jyr}^2})= E(\frac{\partial^2 \log M}{\partial \gamma_{jyr}\partial a_{jr}})= \sum_{g=1}^{G} \sum_{k=1}^p  -\frac{n_{gy} q_{gr}^2(\omega_{jy(k-1)}-\omega_{jyk})}{P_{jyk\mid q_{g}}}.
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
E(\frac{\partial^2 \log M}{\partial \gamma_{jyr}\partial d_{jk}})=\sum_{g=1}^{G} n_{gy} \omega_{jyk}q_{gr}[\frac{1}{P_{jyk\mid q_{g}}}(\omega_{jy(k-1)}-\omega_{jyk}) +\frac{1}{P_{jy(k+1)\mid q_{g}}}(\omega_{jyk}-\omega_{jy(k+1)})].
\end{aligned}
\end{equation}

### Cyclical coordinate descent

Same as in 3.2, to minimize our objective function with respect to $\boldsymbol \Gamma$, our lasso estimator in (13) can be written by

\begin{equation}
\begin{aligned}
\hat{\boldsymbol \Gamma}&=\mathrm{arg} \mathrm{min} \lbrace -\log M(\boldsymbol \Gamma) +  \eta || \boldsymbol \Gamma ||_1 \rbrace\\
&= \mathrm{arg} \mathrm{min} \lbrace -\log M(\boldsymbol \Gamma_0)- \partial_{\Gamma} \log M(\boldsymbol \Gamma_0)(\boldsymbol \Gamma-\boldsymbol \Gamma_0)-\frac{\partial^2_{\Gamma} \log M(\boldsymbol \Gamma_0)}{2}(\boldsymbol \Gamma-\boldsymbol \Gamma_0)^2 +  \eta || \boldsymbol \Gamma ||_1 \rbrace\\
&=-\frac{\mathrm{soft}(\partial_{\boldsymbol \Gamma}\log M-\boldsymbol \Gamma_j^{(t-1)}*\partial^2_{\boldsymbol \Gamma}\log M,\eta)}{\partial^2_{\boldsymbol \Gamma}\log M}
\end{aligned}
\end{equation}




We run a cyclical coordinate descent algorithm for each group (item) with all other groups fixed. For item *j*, our algorithm is given by following.

1. Calculate $P_{jky\mid q_{g}}^{*}$ and $Q_{jky\mid q_{g}}^{*}$.

2. The parameter $a_{jr}$ and $d_{jk}$ can be updated by

$${a}_{jr}^{(t)}=  a_{jr}^{(t-1)}-\frac{\partial_{ a_{jr}}\log M}{\partial^2_{ a_{jr}}\log M},$$

$${d}_{jk}^{(t)}=  d_{jk}^{(t-1)}-\frac{\partial_{ d_{jk}}\log M}{\partial^2_{ d_{jk}}\log M}$$

and

$$\hat{\boldsymbol \Gamma}_{jyr}=-\frac{\mathrm{soft}(\partial_{\boldsymbol \Gamma_{jyr}}\log M-\boldsymbol \Gamma_{jyr}^{(t-1)}*\partial^2_{\boldsymbol \Gamma_{jyr}}\log M,\eta)}{\partial^2_{\boldsymbol \Gamma_{jyr}}\log M}$$

Then we update $P_{jyk\mid q_{g}}^{*}$ and $Q_{jyk\mid q_{g}}^{*}$ by plugging in $\hat{\boldsymbol A}$,$\hat{\boldsymbol D}$ and $\hat{\boldsymbol \beta}$ from last coordinate descent cycle and repeat above steps until a convergence criterion is met.

After we get optimizers for item *j*, we do transforamtions on all estimates as following

$${a}_{jr}^{(t)*}=  {a}_{jr}^{(t)}*\sqrt{\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})},$$

$${\gamma}_{jr}^{(t)*}=  {\gamma}_{jr}^{(t)}*\sqrt{\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})},$$
We run an EM and cyclical coordinate descent algorithm given by following.

\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\boldsymbol A_0$,$\boldsymbol D_0$,$\boldsymbol \Gamma_0$,$\boldsymbol \mu_0$,$\boldsymbol \Sigma_0$,U,$\eta$,$\varepsilon_1$,$\varepsilon_2$}
\Output{$\hat{\boldsymbol A}$,$\hat{\boldsymbol D}$,$\hat{\boldsymbol \Gamma}$,$\hat{\boldsymbol \mu}$,$\hat{\boldsymbol \Sigma}$}
\BlankLine
set $t_1=1$, $\delta^{(t_1-1)}=$ any value greater than $\varepsilon_1$\;
\While{$\delta_1^{(t_1-1)}>\varepsilon_1$}{
    Calculate $n_{gy}$ and $r_{gjyk}$\;
    Update $\boldsymbol \mu^{(t_1)}$ and $\boldsymbol \Sigma^{(t_1)}$\;
    \For {j=1,...m}{
    set $t_2=1$, $\delta_2^{(t_2-1)}=$ any value greater than $\varepsilon_2$\;
    \While{$\delta_2^{(t_2-1)}>\varepsilon_2$}{
        Calculate $P_{jyk\mid q_{g}}^{*},Q_{jyk\mid q_{g}}^{*}$\;
        ${a}_{jr}^{(t_2)}=  a_{jr}^{(t_2-1)}-\frac{\partial_{ a_{jr}}\log M}{\partial^2_{ a_{jr}}\log M}$\;
        ${d}_{jk}^{(t_2)}=  d_{jk}^{(t_2-1)}-\frac{\partial_{ d_{jk}}\log M}{\partial^2_{ d_{jk}}\log M}$\;
        $\Gamma^{(t_2)}_{jyr}=-\frac{\mathrm{soft}(\partial_{\Gamma_{jyr}}\log M- \Gamma_{jyr}^{(t_2-1)}*\partial^2_{\boldsymbol \Gamma_{jyr}}\log M,\eta)}{\partial^2_{\Gamma_{jyr}}\log M}$\;
        $\delta_2^{(t_2)}=||\boldsymbol A_j^{(t_2)}-\boldsymbol A_{j}^{(t_2-1)}||+||\boldsymbol D_j^{(t_2)}-\boldsymbol D_{j}^{(t_2-1)}||+||\boldsymbol \Gamma_j^{(t_2)}-\boldsymbol \Gamma_{j}^{(t_2-1)}||$\;
        $t_2=t_2+1$\; 
    }
    ${a}_{jr}^{(t_1)*}=  {a}_{jr}^{(t_1)}*\sqrt{\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})}$\;
    ${\Gamma}_{jr}^{(t_1)*}=  {\Gamma}_{jr}^{(t_1)}*\sqrt{\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})}$\;
    }
   $\delta_1^{(t_1)}=||\boldsymbol A^{(t_1)}-\boldsymbol A^{(t_1-1)}||+||\boldsymbol D^{(t_1)}-\boldsymbol D^{(t_1-1)}||+||\boldsymbol \Gamma^{(t_1)}-\boldsymbol \Gamma^{(t_1-1)}||$\;
   $t_1=t_1+1$\; 
}
\caption{Non-uniform DIF Detection via LASSO}
\end{algorithm} 

Note that $\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})$ is the *r*th element on the diagonal of the estimated covariance matrix of the reference group $\hat{\boldsymbol {\boldsymbol\Sigma}_1}$.

$\varepsilon_1=10^{-3}$ and $\varepsilon_2=10^{-7}$.

## Tuning Parameter Selectiion

The BIC can be calculated by
$$BIC_{\hat{\textbf A},\hat{\textbf D},\hat{\boldsymbol\Gamma}}=-2\max_{\hat{\textbf A},\hat{\textbf D},\hat{\boldsymbol\Gamma}}\log M+ \|\hat{\boldsymbol\Gamma}\|_0 \log N, \tag{11}$$
where N is the sample size. 

## Simulation

### Simulation 5

$\emph Sample$ $\emph Size.$ The total sample size is $N=1500$, and the group sample sizes are $N_1=N_2=N_3=500$.

$\emph Test$ $\emph Length.$ $m=20.$ Simple structure. 10 items per dimension.

$\emph Proportion \ of \ DIF.$ 4 items with DIF. 2 DIF items per dimension.

$\emph Magnitude \ of \ DIF.$ The first focal group with smaller discrimination parameter (-0.5) on the 4 DIF items. 

The second focal group with much smaller discrimination parameter (-1) on the 4 DIF items.

$\emph Generated \ parameters.$

$a_{j1}\sim U(1.5,2.5), j=1,...,10$

$a_{j2}\sim U(1.5,2.5), j=11,...,20$

$d_{1}\sim N(0,1)$

$$\boldsymbol A=\begin{pmatrix}
2.17&0\\
0& 2.46\\
2.41& 0\\
2.45& 0\\
2.34& 0\\
1.84& 0\\
1.85& 0\\
1.92& 0\\
1.94& 0\\
1.90& 0\\
1.92& 0\\
0& 2.43\\
0& 1.82\\
0& 2.22\\
0& 1.93\\
0& 1.88\\
0& 1.84\\
0& 2.12\\
0& 2.42\\
0& 2.15\\
\end{pmatrix},$$

$$\boldsymbol D=\begin{pmatrix}
0.03\\
-1.28\\
0.58\\
-2.06\\
0.12\\
3.25\\
-0.41\\
-0.51\\
0.89\\
1.33\\
0.85\\
0.82\\
-0.37\\
-0.99\\
-0.27\\
0.19\\
1.73\\
0.05\\
-1.86\\
-0.63\\
\end{pmatrix},$$


$$\boldsymbol\Gamma_j=\begin{pmatrix}
0 & 0\\
0 & 0 \\
\end{pmatrix}, for \ j=1,2,3,6,7,8,9,10,11,14,15,16,17,18,19,20$$ 

$$\boldsymbol\Gamma_j=\begin{pmatrix}
-0.5 & 0\\
-1 & 0 \\
\end{pmatrix}, for \ j=4,5$$ 
$$\boldsymbol\Gamma_j=\begin{pmatrix}
0 & -0.5\\
0 & -1 \\
\end{pmatrix}, for \ j=12,13$$ 

wABC

item | 4 | 5 | 12 | 13 |
------|------|------|------|------|
Focal 1| 0.620 | 0.530 | 0.560 | 0.774 |
Focal 2| 1.331 | 1.242 | 1.292 | 1.813 |

The first two items are used as anchor items in estimation.

No impact. $\theta_i\sim N(\begin{pmatrix}0\\0\\\end{pmatrix}, \begin{pmatrix}1&0.85\\0.85&1\\\end{pmatrix})$

$\emph Tuning \ parameters.$ Tuning parameters $\eta$ are chosen from a sequence starting from 21 to 51 with increment 3.

Results of 50 Replications

$\emph Table \ 19. \ Type \ I \ error \ and \ Power \ of \ regularization \ method$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.625 | 0.14 | 0.625 |
Type I| 0.0314 | 0.0171 | 0.0185 |


$\emph Table \ 20. \ Type \ I \ error \ and \ Power \ of \ mirt \ LRT$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.515 | 0.375 | 0.646 |
Type I| 0.0185 | 0.0238 | 0.0149 |


### Simulation 6

Increase DIF proportion to 60%, and keep everything else the same as simulation 5.

$\emph Sample$ $\emph Size.$ The total sample size is $N=1500$, and the group sample sizes are $N_1=N_2=N_3=500$.

$\emph Proportion \ of \ DIF.$ 12 items with DIF. 6 DIF items per dimension.


$$\boldsymbol\Gamma_j=\begin{pmatrix}
0 & 0\\
0 & 0 \\
\end{pmatrix}, for \ j=1,2,3,10,11,18,19,20$$ 

$$\boldsymbol\Gamma_j=\begin{pmatrix}
-0.5 & 0\\
-1 & 0 \\
\end{pmatrix}, for \ j=4,5,6,7,8,9$$ 
$$\boldsymbol\Gamma_j=\begin{pmatrix}
0 & -0.5\\
0 & -1 \\
\end{pmatrix}, for \ j=12,13,14,15,16,17$$ 

wABC

item | 4 | 5 | 6 | 7 | 8 | 9 | 12 | 13 | 14 | 15 | 16 | 17 |
------|------|------|------|------|------|------|------|------|------|------|------|------|
Focal 1| 0.620 | 0.530 | 0.556 | 0.764 | 0.760 | 0.737 | 0.560 | 0.774 | 0.635 | 0.724 | 0.744 | 0.750 |
Focal 2| 1.331 | 1.242 | 1.076 | 1.788 | 1.771 | 1.686 | 1.292 | 1.813 | 1.452 | 1.702 | 1.751 | 1.589 | 




$\emph Table \ 21. \ Type \ I \ error \ and \ Power \ of \ regularization \ method$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.45 | 0.06 | 0.443 |
Type I| 0.0067 | 0.0033 | 0.0033 |

Adaptive LASSO (selected by GIC)

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.383 | 0.0086 | 0.3836 |
Type I| 0 | 0 | 0 |



$\emph Table \ 22. \ Type \ I \ error \ and \ Power \ of \ mirt \ LRT$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.656 | 0.185 | 0.746 |
Type I| 0.0067 | 0.01 | 0.0233 |



### Simulation 7

Increase total sample size to 3000, and keep everything else the same as simulation 5.

$\emph Sample$ $\emph Size.$ The total sample size is $N=3000$, and the group sample sizes are $N_1=N_2=N_3=1000$.

$\emph Proportion \ of \ DIF.$ 4 items with DIF. 2 DIF items per dimension.

$\emph Table \ 23. \ Type \ I \ error \ and \ Power \ of \ regularization \ method$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.96 | 0.345 | 0.96 |
Type I| 0.0371 | 0.0271 | 0.0214 |


$\emph Table \ 24. \ Type \ I \ error \ and \ Power \ of \ mirt \ LRT$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.815 | 0.555 | 0.885 |
Type I| 0.005 | 0.031 | 0.0119 |


### Simulation 8

Increase DIF proportion to 60%, and keep everything else the same as simulation 7.

$\emph Sample$ $\emph Size.$ The total sample size is $N=3000$, and the group sample sizes are $N_1=N_2=N_3=1000$.

$\emph Proportion \ of \ DIF.$ 12 items with DIF. 6 DIF items per dimension.


$$\boldsymbol\Gamma_j=\begin{pmatrix}
0 & 0\\
0 & 0 \\
\end{pmatrix}, for \ j=1,2,3,10,11,18,19,20$$ 

$$\boldsymbol\Gamma_j=\begin{pmatrix}
-0.5 & 0\\
-1 & 0 \\
\end{pmatrix}, for \ j=4,5,6,7,8,9$$ 
$$\boldsymbol\Gamma_j=\begin{pmatrix}
0 & -0.5\\
0 & -1 \\
\end{pmatrix}, for \ j=12,13,14,15,16,17$$ 

$\emph Table \ 25. \ Type \ I \ error \ and \ Power \ of \ regularization \ method$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.9933 | 0.481 | 0.9933 |
Type I| 0.0333 | 0.02 | 0.02 |

Adaptive LASSO (selected by GIC)

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.866 | 0.112 | 0.866 |
Type I| 0.0083 | 0.0083 | 0 |



$\emph Table \ 26. \ Type \ I \ error \ and \ Power \ of \ mirt \ LRT$

Group | Omnibus DIF| Group with DIF=0.5| Group with DIF=1| 
------|------|------|------|
Power| 0.9367 | 0.3 | 0.9683 |
Type I| 0.0233 | 0.0133 | 0.0333 |


# Non-uniform DIF Detection via Group LASSO 

When the items have non-uniform DIF on both slope and intercept, the DIF parameter we are estimating are $\boldsymbol \Gamma = (\boldsymbol 0,..., \boldsymbol 0, \boldsymbol \Gamma_{q+1},...,\boldsymbol \Gamma_{m})$ and $\boldsymbol \beta = (\boldsymbol 0,..., \boldsymbol 0, \boldsymbol \beta_{q+1},...,\boldsymbol \beta_{m})$.

## E step

The equations for expectations are same as before.

In our DIF detection problem, we minimize the following objective function 

\begin{equation}
\begin{aligned}
-\log M + \eta \sum_j^m ||(\boldsymbol \Gamma_j, \boldsymbol \beta_j )||_2 
\end{aligned}
\end{equation}

For each item, we minimize

\begin{equation}
\begin{aligned}
-\log M_j +  \eta ||(\boldsymbol \Gamma_j, \boldsymbol \beta_j )||_2 
\end{aligned}
\end{equation}


where $\eta$ is the group lasso tuning parameter.

We denote by $\boldsymbol\tau \in \mathbb R^{(y-1)*r+(y-1)*(m-1)}$ the whole DIF parameter vector, i.e. $\boldsymbol\tau=(\boldsymbol\Gamma,\boldsymbol\beta)^T$.

Then, our objective function is

\begin{equation}
\begin{aligned}
S_\eta(\boldsymbol\tau)=-\log M +  \eta \sum_j^m ||\boldsymbol\tau||_2.
\end{aligned}
\end{equation}

For each item *j*,

\begin{equation}
\begin{aligned}
S_\eta(\boldsymbol\tau_j)=-\log M_j +  \eta ||\boldsymbol\tau_j||_2.
\end{aligned}
\end{equation}


## M step

The equations are same as in section 3.2. The Block co-ordinate gradient descent for solving the group lasso problem as follow.

### Block co-ordinate gradient descent

Using a second-order Taylor series expansion at $\hat{\boldsymbol\tau}^{(t-1)}$ we define

$$M_{\eta}^{(t-1)}(\boldsymbol \epsilon^{(t)})=-\lbrace\log M + \boldsymbol \epsilon^{(t)T} \nabla\log M+\frac{1}{2}\boldsymbol \epsilon^{(t)T}H^{(t-1)}\boldsymbol \epsilon^{(t)} \rbrace+  \eta \sum_j^m ||\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon_j^{(t)}||_2,$$

where $\boldsymbol\tau^{(t)}=\boldsymbol\tau^{(t-1)}+\boldsymbol \epsilon^{(t)}$, and $$\nabla\log M=(\frac{\partial \log M}{\partial \boldsymbol \Gamma},\frac{\partial \log M}{\partial \boldsymbol \beta})$$ and 

$$H^{(t-1)}=
\begin{pmatrix}
\frac{\partial^2 \log M}{\partial \boldsymbol \Gamma^2} & \frac{\partial^2 \log M}{\partial \boldsymbol \Gamma \partial \boldsymbol \beta}\\
\frac{\partial^2 \log M}{\partial \boldsymbol \Gamma \partial \boldsymbol \beta} & \frac{\partial^2 \log M}{\partial \boldsymbol \beta^2}\\
\end{pmatrix}.$$

We have $M_{\eta}^{(t-1)}(\boldsymbol \epsilon)\approx S_\eta(\hat{\boldsymbol\tau}^{(t-1)}+\boldsymbol \epsilon^{(t)})$.

We run a block co-ordinate gradient descent algorithm for each group (item) with all other groups fixed. 

For item *j*, denote $u$ to be the subgradient of $||\boldsymbol\tau_j^{(t-1)}+\boldsymbol \epsilon_j^{(t)}||_2$. We have

$$u =\begin{cases} \frac{\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon_j^{(t)}}{||\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon_j^{(t)}||_2}, & \mbox{if } \hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon_j^{(t)} \neq \boldsymbol 0 \\ \in\lbrace u: ||u||_2\leq1\rbrace, & \mbox{if } \hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon_j^{(t)} = \boldsymbol 0 \end{cases}.$$

The subgradient equation $\partial_{\epsilon_j} M_{\eta}^{(t-1)}(\boldsymbol \epsilon^{(t)})=-\nabla\log M_j-\boldsymbol \epsilon^{(t)T}_jH^{(t-1)}_{j}+\eta u=0$ is satisfied with $\boldsymbol\tau_j^{(t-1)}+\boldsymbol \epsilon_j=0$ if

$$||u||_2=||\frac{\nabla\log M_j+\boldsymbol \epsilon^{(t)T}_jH^{(t-1)}_{j}}{\eta}||_2 \leq1$$


$$||\nabla\log M_j+\boldsymbol \epsilon^{(t)T}_jH^{(t-1)}_{j}||_2 \leq\eta$$

$$||\nabla\log M_j-\hat{\boldsymbol\tau}_j^{(t-1)}H^{(t-1)}_{j}||_2 \leq\eta,$$

the minimizer of $M_{\eta}^{(t-1)}(\boldsymbol \epsilon)$ is 

$$\hat{\boldsymbol \epsilon}^{(t)}_j=-\hat{\boldsymbol\tau}_j^{(t-1)}.$$

Otherwise,

Then the subgradient equation is 

\begin{equation}
\begin{aligned}
\partial_{\epsilon_j} M_{\eta}^{(t-1)}(\boldsymbol \epsilon^{(t)})=-\nabla\log M_j-\boldsymbol \epsilon^{(t)T}_jH^{(t-1)}_{j}+\eta \frac{\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j}{||\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j||_2}=0
\end{aligned}
\end{equation}


$$\partial_{\epsilon_j} M_{\eta}^{(t-1)}(\boldsymbol \epsilon^{(t)})=-\nabla\log M_j-(\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j) H^{(t-1)}_{j}+ \hat{\boldsymbol\tau}_j^{(t-1)} H^{(t-1)}_{j} +\eta \frac{\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j}{||\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j||_2}=0$$

\begin{equation}
\begin{aligned}
\nabla\log M_j - \hat{\boldsymbol\tau}_j^{(t-1)} H^{(t-1)}_{j}= -(\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j) H^{(t-1)}_{j} +\eta \frac{\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j}{||\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j||_2}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\boldsymbol \tau = \hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j = \frac{(\nabla\log M_j - \hat{\boldsymbol\tau}_j^{(t-1)} H^{(t-1)}_{j})||\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j||_2}{\eta-H^{(t-1)}_{j}||\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j||_2}
\end{aligned}
\end{equation}


Taking the norm of both sides of (57) we see that

$$||\nabla\log M_j - \hat{\boldsymbol\tau}_j^{(t-1)} H^{(t-1)}_{j}||_2=( \frac{\eta}{||\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j||_2} - H^{(t-1)}_{j}) ||\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j||_2$$

\begin{equation}
\begin{aligned}
||\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j||_2=\frac{\eta-||\nabla\log M_j - \hat{\boldsymbol\tau}_j^{(t-1)} H^{(t-1)}_{j}||_2}{H^{(t-1)}_{j}} 
\end{aligned}
\end{equation}

Plugging (59) into (58), we have

\begin{equation}
\begin{aligned}
\boldsymbol \epsilon^{(t)}_j=-(H^{(t-1)}_{j})^{-1}\lbrace\nabla\log M_j-\eta \frac{\nabla\log M_j-\hat{\boldsymbol\tau}_j^{(t-1)}H^{(t-1)}_{j}}{||\nabla\log M_j-\hat{\boldsymbol\tau}_j^{(t-1)}H^{(t-1)}_{j}||_2}\rbrace.
\end{aligned}
\end{equation}


$$\nabla\log M_j=(\frac{\partial \log M}{\partial \gamma_{j11}},...,\frac{\partial \log M}{\partial \gamma_{jyr}},...,\frac{\partial \log M}{\partial \gamma_{j3q}},\frac{\partial \log M}{\partial \beta_{j11}},...,\frac{\partial \log M}{\partial \beta_{jyk}},...\frac{\partial \log M}{\partial \beta_{j3(p-1)}}), r=1,...,q; k=1,...,p-1; y=2,3.$$

If $\boldsymbol \epsilon^{(t)}_j\neq0$, performing a Backtracking-Armijo line search: let $\alpha^{(t)}$ be the largest value in $\lbrace\alpha_0\delta^l\rbrace_{l\geq 0}$ s.t.

\begin{equation}
\begin{aligned}
S_{\eta}(\hat{\boldsymbol\tau}^{(t-1)}_j+\alpha^{(t)} \boldsymbol \epsilon^{(t)}_j)-S_{\eta}(\hat{\boldsymbol\tau}^{(t-1)}_j)\leq\alpha^{(t)}\sigma\Delta^{(t)},
\end{aligned}
\end{equation}


where $\alpha_0=1$, $\delta=0.5$ and $\sigma=0.1$, and $\Delta$ is the improvement in the objective function $S_{\eta}(\cdot)$ when using a linear approximation for the log-likelihood, i.e.

\begin{equation}
\begin{aligned}
\Delta^{(t)}_j=-\boldsymbol \epsilon^{(t)T}_j\nabla\log M_j+\eta||\hat{\boldsymbol\tau}_j^{(t-1)}+\boldsymbol \epsilon^{(t)}_j||_2-\eta||\hat{\boldsymbol\tau}_j^{(t-1)}||_2.
\end{aligned}
\end{equation}


$$\hat{\boldsymbol\tau}^{(t)}_j=(\hat{\boldsymbol\Gamma}^{(t)}_j,\hat{\boldsymbol\beta}^{(t)}_j)=\hat{\boldsymbol\tau}^{(t-1)}_j+\alpha^{(t)}\boldsymbol \epsilon^{(t)}_j$$



Then we update $P_{jky\mid q_{g}}^{*}$ and $Q_{jky\mid q_{g}}^{*}$ by plugging in $\hat{\boldsymbol A}$,$\hat{\boldsymbol D}$, $\hat{\boldsymbol \Gamma}$ and $\hat{\boldsymbol \beta}$ from last coordinate descent cycle and repeat above steps until a convergence criterion is met.

After we get optimizers for item *j*, we do transforamtions on all estimates as following

$${a}_{jr}^{(t)*}=  {a}_{jr}^{(t)}*\sqrt{\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})},$$

$${\gamma}_{jr}^{(t)*}=  {\gamma}_{jr}^{(t)}*\sqrt{\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})},$$


where $\mu_{1r}$ is the *r*th element of the estimated mean vector of the reference group $\hat{\boldsymbol \mu_1}$, and $\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})$ is the *r*th element on the diagonal of the estimated covariance matrix of the reference group $\hat{\boldsymbol {\boldsymbol\Sigma}_1}$.


We run an EM and block coordinate gradient descent algorithm given by following.

\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\boldsymbol A_0$,$\boldsymbol D_0$,$\boldsymbol \Gamma_0$,$\boldsymbol \beta_0$,$\boldsymbol \mu_0$,$\boldsymbol \Sigma_0$,U,$\eta$,$\varepsilon_1$,$\varepsilon_2$}
\Output{$\hat{\boldsymbol A}$,$\hat{\boldsymbol D}$,$\hat{\boldsymbol \Gamma}$,$\boldsymbol \beta$,$\hat{\boldsymbol \mu}$,$\hat{\boldsymbol \Sigma}$}
\BlankLine
set $t_1=1$, $\delta^{(t_1-1)}=$ any value greater than $\varepsilon_1$\;
\While{$\delta_1^{(t_1-1)}>\varepsilon_1$}{
    Calculate $n_{gy}$ and $r_{gjyk}$\;
    Update $\boldsymbol \mu^{(t_1)}$ and $\boldsymbol \Sigma^{(t_1)}$\;
    \For {j=1,...m}{
    set $t_2=1$, $\delta_2^{(t_2-1)}=$ any value greater than $\varepsilon_2$\;
    \While{$\delta_2^{(t_2-1)}>\varepsilon_2$}{
        Calculate $P_{jyk\mid q_{g}}^{*},Q_{jyk\mid q_{g}}^{*}$\;
        ${a}_{jr}^{(t_2)}=  a_{jr}^{(t_2-1)}-\frac{\partial_{ a_{jr}}\log M}{\partial^2_{ a_{jr}}\log M}$\;
        ${d}_{jk}^{(t_2)}=  d_{jk}^{(t_2-1)}-\frac{\partial_{ d_{jk}}\log M}{\partial^2_{ d_{jk}}\log M}$\;
        \eIf{$||\nabla\log M_j-\hat{\boldsymbol\tau}_j^{(t_2-1)}H^{(t_2-1)}_{j}||_2 \leq\eta$}{
        ${\boldsymbol \tau}_{j}=\boldsymbol 0$\;
        }{$\boldsymbol \epsilon_j^{(t_2)}=-(H^{(t_2-1)}_{j})^{-1}\lbrace\nabla\log M_j-\eta \frac{\nabla\log M_j-\hat{\boldsymbol\tau}_j^{(t_2-1)}H^{(t_2-1)}_{j}}{||\nabla\log M_j-\hat{\boldsymbol\tau}_j^{(t_2-1)}H^{(t_2-1)}_{j}||_2}\rbrace$\;
        $\Delta_j^{(t_2)}=-\boldsymbol \epsilon_j^{(t_2)T}\nabla\log M_j+\eta||\hat{\boldsymbol\tau}_j^{(t_2-1)}+\boldsymbol \epsilon_j^{(t_2)}||_2-\eta||\hat{\boldsymbol\tau}_j^{(t_2-1)}||_2$\;
        $\alpha^{(t_2)}$ is the max value in $\lbrace\alpha^{(0)}\delta^l\rbrace_{l\geq 0}$ such that $S_{\eta}(\hat{\boldsymbol\tau}^{(t_2-1)}_j+\alpha^{(t_2)}\boldsymbol \epsilon^{(t_2)}_j)-S_{\eta}(\hat{\boldsymbol\tau}^{(t_2-1)}_j)\leq\alpha^{(t_2)}\sigma\Delta^{(t_2)}.$\;
        $\boldsymbol\tau^{(t_2)}_j=\boldsymbol\tau^{(t_2-1)}_j+\alpha^{(t_2)}\boldsymbol \epsilon^{(t_2)}_j$\;}
        $\delta_2^{(t_2)}=||\boldsymbol A_j^{(t_2)}-\boldsymbol A_{j}^{(t_2-1)}||+||\boldsymbol D_j^{(t_2)}-\boldsymbol D_{j}^{(t_2-1)}||+||\alpha^{(t_2-1)}\boldsymbol \epsilon^{(t_2-1)}_j||$\;
        $t_2=t_2+1$\; 
    }
    ${a}_{jr}^{(t_1)*}=  {a}_{jr}^{(t_1)}*\sqrt{\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})}$\;
    ${\Gamma}_{jr}^{(t_1)*}=  {\Gamma}_{jr}^{(t_1)}*\sqrt{\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})}$\;
    }
   $\delta_1^{(t_1)}=||\boldsymbol A^{(t_1)}-\boldsymbol A^{(t_1-1)}||+||\boldsymbol D^{(t_1)}-\boldsymbol D^{(t_1-1)}||+||\boldsymbol \Gamma^{(t_1)}-\boldsymbol \Gamma^{(t_1-1)}||+||\boldsymbol \beta^{(t_1)}-\boldsymbol \beta^{(t_1-1)}||$\;
   $t_1=t_1+1$\; 
}
\caption{Non-uniform DIF Detection via group LASSO}
\end{algorithm} 


Note that $\mathrm{diag}(\hat{\boldsymbol\Sigma}_{1r})$ is the *r*th element on the diagonal of the estimated covariance matrix of the reference group $\hat{\boldsymbol {\boldsymbol\Sigma}_1}$.

$\varepsilon_1=10^{-3}$ and $\varepsilon_2=10^{-7}$.

## Tuning Parameter Selection

The Bayesian information criterion (BIC) in Tutz et al.(2015) is applied for tuning parameter selection. The degrees of freedom of penalized parameters $\boldsymbol\tau=(\boldsymbol\Gamma, \boldsymbol\beta)^T$ are approximated by

$$\tilde{df}_{\boldsymbol\tau}(\eta)=\sum_j^mI(||\boldsymbol\tau_j(\eta)||_2>0)+\sum_j^m\frac{||\boldsymbol\tau_j(\eta)||_2}{||\boldsymbol\tau_j^{ML}||_2}(m'-1)$$

where $m'$ is the number of DIF parameters for each item. Here we have $m'=4$.

The total degrees of freedom are

$$df(\eta)=m+N+\tilde{df}_{\boldsymbol\tau}(\eta)-1.$$

The BIC can be calculated by
$$BIC(\eta)=-2\max_{\hat{\textbf A},\hat{\textbf D},\hat{\boldsymbol\tau}}\log M+ df(\eta) \log (m\cdot N).$$
where N is the sample size. The first term control the bias of the estimator, and the second term penalize the complexity of the model. We want to choose a tuning parameter $\eta$ which gives us the smallest BIC value.

## Simulation

### Simulation 9

$\emph Sample$ $\emph Size.$ The total sample size is $N=1500$, and the group sample sizes are $N_1=N_2=N_3=500$.

$\emph Test$ $\emph Length.$ $m=20.$ Simple structure. 10 items per dimension.

$\emph Proportion \ of \ DIF.$ 4 items with DIF. 2 DIF items per dimension.

$\emph Magnitude \ of \ DIF.$ The first focal group with smaller discrimination parameter (-0.5) and larger difficulty parameters (+0.5) on the 4 DIF items. 

The second focal group with much smaller difficulty parameter (-1) and much larger difficulty parameters (+1) on the 4 DIF items.

$\emph Generated \ parameters.$

$a_{j1}\sim U(1.5,2.5), j=1,...,10$

$a_{j2}\sim U(1.5,2.5), j=11,...,20$

$d_{1}\sim N(0,1)$

$$\boldsymbol A=\begin{pmatrix}
2.17&0\\
0& 2.46\\
2.41& 0\\
2.45& 0\\
2.34& 0\\
1.84& 0\\
1.85& 0\\
1.92& 0\\
1.94& 0\\
1.90& 0\\
1.92& 0\\
0& 2.43\\
0& 1.82\\
0& 2.22\\
0& 1.93\\
0& 1.88\\
0& 1.84\\
0& 2.12\\
0& 2.42\\
0& 2.15\\
\end{pmatrix},$$

$$\boldsymbol D=\begin{pmatrix}
0.03\\
-1.28\\
0.58\\
-2.06\\
0.12\\
3.25\\
-0.41\\
-0.51\\
0.89\\
1.33\\
0.85\\
0.82\\
-0.37\\
-0.99\\
-0.27\\
0.19\\
1.73\\
0.05\\
-1.86\\
-0.63\\
\end{pmatrix},$$


$$\boldsymbol\Gamma_j=\begin{pmatrix}
0 & 0\\
0 & 0 \\
\end{pmatrix}, for \ j=1,2,3,6,7,8,9,10,11,14,15,16,17,18,19,20$$ 

$$\boldsymbol\Gamma_j=\begin{pmatrix}
-0.3 & 0\\
-0.6 & 0 \\
\end{pmatrix}, for \ j=4,5$$ 
$$\boldsymbol\Gamma_j=\begin{pmatrix}
0 & -0.3\\
0 & -0.6 \\
\end{pmatrix}, for \ j=12,13$$ 

$$\boldsymbol\beta=\begin{pmatrix}
0 & 0\\
0 & 0 \\
0 & 0 \\
0.5 & 1 \\
0.5 & 1 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0.5 & 1 \\
0.5 & 1 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
\end{pmatrix},$$

The first two items are used as anchor items in estimation.

No impact. $\theta_i\sim N(\begin{pmatrix}0\\0\\\end{pmatrix}, \begin{pmatrix}1&0.85\\0.85&1\\\end{pmatrix})$

$\emph Tuning \ parameters.$ Tuning parameters $\eta$ are chosen from a sequence starting from 21 to 51 with increment 3.

Results of 50 Replications

$\emph Table \ 27. \ Type \ I \ error \ and \ Power \ of \ regularization \ method$

Group | Omnibus DIF| 
------|------|
Power| 0.93 | 
Type I| 0.0457 | 

$\emph Table \ 28. \ Type \ I \ error \ and \ Power \ of \ mirt \ LRT$

Group | Omnibus DIF| 
------|------|
Power| 0.925 | 
Type I| 0.0142 | 

### Simulation 10

Keep everything the same as simulation 9. Increase the DIF proportion to 60%.
$\emph Sample$ $\emph Size.$ The total sample size is $N=1500$, and the group sample sizes are $N_1=N_2=N_3=500$.

$\emph Proportion \ of \ DIF.$ 12 items with DIF. 6 DIF items per dimension.


$$\boldsymbol\Gamma_j=\begin{pmatrix}
0 & 0\\
0 & 0 \\
\end{pmatrix}, for \ j=1,2,3,10,11,18,19,20$$ 

$$\boldsymbol\Gamma_j=\begin{pmatrix}
-0.3 & 0\\
-0.6 & 0 \\
\end{pmatrix}, for \ j=4,5,6,7,8,9$$ 
$$\boldsymbol\Gamma_j=\begin{pmatrix}
0 & -0.3\\
0 & -0.6 \\
\end{pmatrix}, for \ j=12,13,14,15,16,17$$ 

$$\boldsymbol\beta=\begin{pmatrix}
0 & 0\\
0 & 0 \\
0 & 0 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0 & 0 \\
0 & 0 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0.5 & 1 \\
0 & 0 \\
0 & 0 \\
0 & 0 \\
\end{pmatrix},$$

Results of 50 Replications

$\emph Table \ 29. \ Type \ I \ error \ and \ Power \ of \ regularization \ method$

Group | Omnibus DIF| 
------|------|
Power| 0.842 | 
Type I| 0.0033 | 

$\emph Table \ 30. \ Type \ I \ error \ and \ Power \ of \ mirt \ LRT$

Group | Omnibus DIF| 
------|------|
Power| 0.926 | 
Type I| 0.023 | 


### Simulation 11

Keep everything the same as simulation 9. Increase the sample size.
$\emph Sample$ $\emph Size.$ The total sample size is $N=3000$, and the group sample sizes are $N_1=N_2=N_3=1000$.

$\emph Proportion \ of \ DIF.$ 4 items with DIF. 2 DIF items per dimension.

Results of 50 Replications

$\emph Table \ 31. \ Type \ I \ error \ and \ Power \ of \ regularization \ method$

Group | Omnibus DIF| 
------|------|
Power| 1 | 
Type I| 0.0557 | 

$\emph Table \ 32. \ Type \ I \ error \ and \ Power \ of \ mirt \ LRT$

Group | Omnibus DIF| 
------|------|
Power| 0.985 | 
Type I| 0.008 | 

### Simulation 12

Keep everything the same as simulation 10. Increase the sample size to 3000.
$\emph Sample$ $\emph Size.$ The total sample size is $N=3000$, and the group sample sizes are $N_1=N_2=N_3=1000$.

$\emph Proportion \ of \ DIF.$ 12 items with DIF. 6 DIF items per dimension.

Results of 50 Replications

$\emph Table \ 33. \ Type \ I \ error \ and \ Power \ of \ regularization \ method$

Group | Omnibus DIF| 
------|------|
Power| 0.941 | 
Type I| 0.0459 | 

$\emph Table \ 34. \ Type \ I \ error \ and \ Power \ of \ mirt \ LRT$

Group | Omnibus DIF| 
------|------|
Power| 0.985 | 
Type I| 0.023 | 

# Plots of Results

## DIF only on intercept

```{r echo=FALSE}
N.list=c(1500,3000)
power.list=c(0.985,1)
plot(N.list, power.list,type='l',xlab='N',ylab='Power',ylim= c(0.85,1),col="salmon",xaxt = 'n')
axis(1, at = N.list, labels = N.list)
text(N.list, power.list, power.list, cex=0.8,col="salmon")
lines(N.list,c(0.963,0.996),col="plum")
text(N.list, c(0.963,0.996), c(0.963,0.996), cex=0.8,col="plum",pch=19)
lines(N.list,c(0.872,0.998),col="skyblue")
text(N.list, c(0.872,0.998), c(0.872,0.998), cex=0.8,col="skyblue",pch=19)
lines(N.list,c(0.86,0.995),col="blue")
text(N.list, c(0.86,0.995), c(0.86,0.995), cex=0.8,col="blue",pch=19)
legend(2700, 0.9, legend=c("Reg 20%", "Reg 60%", "LRT 20%","LRT 60%"),col=c("salmon", "skyblue","blue","plum"), lty=1, cex=0.8)
```

```{r echo=FALSE}
N.list=c(1500,3000)
tpI.list=c(0.047,0.051)
plot(N.list, tpI.list,type='l',xlab='N',ylab='Type I',ylim= c(0,0.14),col="salmon",xaxt = 'n')
axis(1, at = N.list, labels = N.list)
text(N.list, tpI.list, tpI.list, cex=0.8,col="salmon")
lines(N.list,c(0.135,0.05),col="skyblue")
text(N.list, c(0.135,0.05), c(0.135,0.05), cex=0.8,col="skyblue",pch=19)
lines(N.list,c(0.007,0.008),col="blue")
text(N.list, c(0.007,0.008), c(0.007,0.008), cex=0.8,col="blue",pch=19)
lines(N.list,c(0.0175,0.02),col="plum")
text(N.list, c(0.0175,0.02), c(0.0175,0.02), cex=0.8,col="plum",pch=19)
legend(2700, 0.12, legend=c("Reg 20%", "Reg 60%", "LRT 20%","LRT 60%"),col=c("salmon", "skyblue","blue","plum"), lty=1, cex=0.8)

```

## DIF only on slope

```{r echo=FALSE}
N.list=c(1500,3000)
power.list=c(0.625,0.96)
plot(N.list, power.list,type='l',xlab='N',ylab='Power',ylim= c(0.44,1),col="salmon",xaxt = 'n')
axis(1, at = N.list, labels = N.list)
text(N.list, power.list, power.list, cex=0.8,col="salmon")
lines(N.list,c(0.45,0.993),col="skyblue")
text(N.list, c(0.45,0.993), c(0.45,0.993), cex=0.8,col="skyblue",pch=19)
lines(N.list,c(0.515,0.815),col="blue")
text(N.list, c(0.515,0.815), c(0.515,0.815), cex=0.8,col="blue",pch=19)
lines(N.list,c(0.656,0.9367),col="plum")
text(N.list, c(0.656,0.9367), c(0.656,0.9367), cex=0.8,col="plum",pch=19)
legend(2700, 0.6, legend=c("Reg 20%", "Reg 60%", "LRT 20%","LRT 60%"),col=c("salmon", "skyblue","blue","plum"), lty=1, cex=0.8)
```

```{r echo=FALSE}
N.list=c(1500,3000)
tpI.list=c(0.03,0.03)
plot(N.list, tpI.list,type='l',xlab='N',ylab='Type I',ylim= c(0,0.05),col="salmon",xaxt = 'n')
axis(1, at = N.list, labels = N.list)
text(N.list, tpI.list, tpI.list, cex=0.8,col="salmon")
lines(N.list,c(0.0067,0.0033),col="skyblue")
text(N.list, c(0.0067,0.0033), c(0.0067,0.0033), cex=0.8,col="skyblue",pch=19)
lines(N.list,c(0.0185,0.005),col="blue")
text(N.list, c(0.0185,0.005), c(0.0185,0.005), cex=0.8,col="blue",pch=19)
lines(N.list,c(0.0067,0.02),col="plum")
text(N.list, c(0.0067,0.02), c(0.0067,0.02), cex=0.8,col="plum",pch=19)
legend(2700, 0.048, legend=c("Reg 20%", "Reg 60%", "LRT 20%","LRT 60%"),col=c("salmon", "skyblue","blue","plum"), lty=1, cex=0.8)
```

## DIF on slope and intercept

```{r echo=FALSE}
N.list=c(1500,3000)
power.list=c(0.93,1)
plot(N.list, power.list,type='l',xlab='N',ylab='Power',ylim= c(0.82,1),col="salmon",xaxt = 'n')
axis(1, at = N.list, labels = N.list)
text(N.list, power.list, power.list, cex=0.8,col="salmon")
lines(N.list,c(0.842,0.941),col="skyblue")
text(N.list, c(0.842,0.941), c(0.842,0.941), cex=0.8,col="skyblue",pch=19)
lines(N.list,c(0.925,0.985),col="blue")
text(N.list, c(0.925,0.985), c(0.925,0.985), cex=0.8,col="blue",pch=19)
lines(N.list,c(0.926,0.986),col="plum")
text(N.list, c(0.926,0.986), c(0.926,0.986), cex=0.8,col="plum",pch=19)
legend(2700, 0.9, legend=c("Reg 20%", "Reg 60%", "LRT 20%","LRT 60%"),col=c("salmon", "skyblue","blue","plum"), lty=1, cex=0.8)
```

```{r echo=FALSE}
N.list=c(1500,3000)
tpI.list=c(0.045,0.0557)
plot(N.list, tpI.list,type='l',xlab='N',ylab='Type I',ylim= c(0,0.06),col="salmon",xaxt = 'n')
axis(1, at = N.list, labels = N.list)
text(N.list, tpI.list, tpI.list, cex=0.8,col="salmon")
lines(N.list,c(0.0033,0.04),col="skyblue")
text(N.list, c(0.0033,0.04), c(0.0033,0.04), cex=0.8,col="skyblue",pch=19)
lines(N.list,c(0.0142,0.008),col="blue")
text(N.list, c(0.0142,0.008), c(0.0142,0.008), cex=0.8,col="blue",pch=19)
lines(N.list,c(0.023,0.023),col="plum")
text(N.list, c(0.023,0.023), c(0.023,0.023), cex=0.8,col="plum",pch=19)
legend(2200, 0.046, legend=c("Reg 20%", "Reg 60%", "LRT 20%","LRT 60%"),col=c("salmon", "skyblue","blue","plum"), lty=1, cex=0.8)
```

# High DIF Proportion Issue

When DIF proportion is higher than 50%, we found either our original LASSO or group LASSO algorithm work. Both of two methods would give us "reverse" type solutions-- the true DIF parameters would be estimated as zero and non-DIF parameters would be non-zero. Below is a plot of the solution path of LASSO algorithm in 60% DIF on intercept condition. At the selected tuning parameter ($\eta=36$), most DIF items (black lines) are shrunk to zero and non-DIF items (colored lines) are non-zero. 

```{r echo=FALSE}
eta.list=seq(from=0, to=48,by=3)
para.list=c(0.910,0.835,0.500,0.360,0,0,0,0,0,0,0,0,0,0,0,0,0)
plot(eta.list,para.list,type='l',xlab='eta',ylab='DIF Parameters',ylim = c(-1,1))
lines(eta.list,c(0.623,0.551,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0))
lines(eta.list,c(1.031,0.958,0.513,0.515,0.373,0,0,0,0,0,0,0,0,0,0,0,0))
lines(eta.list,c(1.048,0.998,0.774,0.752,0.657,0.628,0.628,0.628,0.583,0.645,0.4405993,0.4413321,0.4412420,0.4412020,0.4412116,0.4415708,0.5395321))
lines(eta.list,c(1.086,1.034,0.802,0.779,0.680,0.648,0.648,0.648,0.603,0.665,0.4491815,0.4497850,0.4496789,0.4496385,0.4496482,0.4500801,0.5496246))
lines(eta.list,c(0.845,0.790,0.545,0.525,0.334,0.301,0.300,0.300,0,0,0,0,0,0,0,0,0))
lines(eta.list,c(1.039,0.965,0.907,0.904,0.557,0.304,0,0,0,0,0,0,0,0,0,0,0))
lines(eta.list,c(0.870,0.816,0.733,0.731,0.479,0.299,0.238,0.249,0.243,0.241,0.2418083,0.2692266,0,0,0,0,0))
lines(eta.list,c(0.897,0.784,0.768,0.766,0.434,0,0,0,0,0,0,0,0,0,0,0,0))
lines(eta.list,c(0.741,0.667,0.653,0.651,0,0,0,0,0,0,0,0,0,0,0,0,0))
lines(eta.list,c(0.931,0.866,0.816,0.813,0.508,0.287,0,0,0,0,0,0,0,0,0,0,0))
lines(eta.list,c(1.138,1.066,0.939,0.936,0.595,0,0,0,0,0,0,0,0,0,0,0,0))
lines(eta.list,c(-0.149,-0.206,-0.461,-0.484,-0.596,-0.623,-0.623,-0.623,-0.677,-0.601,-0.646,-0.431,-0.431,-0.431,-0.431,-0.431,0),col=3)
lines(eta.list,c(0.059,0.006,-0.231,-0.250,-0.351,-0.288,-0.288,-0.288,-0.345,0,0,0,0,0,0,0,0),col=10)
lines(eta.list,c(-0.185,-0.242,-0.501,-0.523,-0.634,-0.656,-0.657,-0.657,-0.716,-0.494,-0.492,-0.492,-0.492,-0.492,-0.492,-0.492,0),col=11)
lines(eta.list,c(-0.364,-0.429,-0.478,-0.481,-0.772,-0.977,-1.068,-1.068,-0.868,-0.872,-0.872,-0.841,-0.878,-0.878,-0.878,-0.796,-0.796),col=18)
lines(eta.list,c(-0.357,-0.426,-0.473,-0.476,-0.774,-0.978,-1.065,-1.065,-1.011,-0.802,-0.802,-0.773,-0.815,-0.815,-0.815,-0.708,-0.708),col=19)
lines(eta.list,c(-0.062,-0.133,-0.184,-0.187,-0.506,-0.727,-0.823,-0.822,-0.602,-0.606,-0.607,-0.574,-0.616,-0.616,-0.616,0,0),col=20)
abline(v=36)
```

To solve this issue, we consider following methods:

1. "EMM" algorithm 

From the above solution path plot, we can see when the tuning parameter is zero, the parameter estimations are good (DIF parameters are large and non-DIF parameters are close to zero). But the bias of LASSO estimates increases as tuning parameter increasing. So we considered adding a re-estimation step, that is, perform another M-step without penalty term after each EM cycle. The results of simulation 2, 4, 6, 8, 10 and 12 in this document are estimated by EMM algorithm and they look acceptable. 

2. Using a different constraint

In section 4, we mentioned using anchor items for model identification. Those anchor items have no DIF on all focal groups. Here we can use a different constraint by setting "group-specific" anchors. Suppose we have two focal groups. Two anchor items can be used and each of them has DIF on one focal group but has no DIF on the other focal group. 

To be more specific, recall
$$\boldsymbol \beta_j =(\boldsymbol \beta_{j1\cdot},\boldsymbol \beta_{j2\cdot})^T
=\begin{pmatrix}
\beta_{j11} & \beta_{j12} & ...&\beta_{j1k} & ... & \beta_{j1,p-1}\\
\beta_{j21} & \beta_{j22} & ...&\beta_{j2k} & ... & \beta_{j2,p-1}\\
\end{pmatrix}(k=1,...,p-1),$$

where *p* is the number of categories in GRM, and each row of $\boldsymbol \beta_{j}$ is (uniform) DIF parameter for a focal group, i.e. $\boldsymbol \beta_{j1\cdot}$ is (uniform) DIF parameter for the first focal group, and $\boldsymbol \beta_{j2\cdot}$ is (uniform) DIF parameter for the second focal group.

If the first two items are anchors, we want to set $\beta_{j11}=0$ and $\beta_{j22}=0$, and $\beta_{j21}$ and $\beta_{j12}$ be non-zero and their magnitudes can be freely estimated. This constraint strongly relies on a priori knowledge of the items and the entire study but it works well when the DIF proportion is high.

3. Adaptive LASSO 

Pick a $\lambda >0$, and define the weight vector $$\hat w =1/{|\boldsymbol{\hat{\beta}|}^\lambda}$$ ($\boldsymbol{\hat{\beta}}$ can be the MLE).

\begin{equation}
\begin{aligned}
(\hat{\boldsymbol A},\hat{\boldsymbol D},\hat{\boldsymbol \beta})=\mathrm{arg} \mathrm{min} \lbrace -\log M +  \eta_n \sum_j^m \hat w_j || \boldsymbol \beta_j ||_1  \rbrace
\end{aligned}
\end{equation}

Works. 

4. SCAD penalty (Fan & Li, 2001)


